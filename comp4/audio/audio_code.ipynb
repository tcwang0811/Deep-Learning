{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11785,
     "status": "ok",
     "timestamp": 1578836368005,
     "user": {
      "displayName": "Yang Lo",
      "photoUrl": "",
      "userId": "07530317156869475190"
     },
     "user_tz": -480
    },
    "id": "cw7MVWoMwkXx",
    "outputId": "38710640-792f-4bc7-c3b4-d3955a1ec179"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Install TensorFlow\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import gc\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10748,
     "status": "ok",
     "timestamp": 1578836368011,
     "user": {
      "displayName": "Yang Lo",
      "photoUrl": "",
      "userId": "07530317156869475190"
     },
     "user_tz": -480
    },
    "id": "lh6wyU_HwkYM",
    "outputId": "c22ba4bb-3a78-4db0-b4e1-fd1120f25cc2"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYvPdtCWwkYQ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "BUFFER_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kmdAv6zwkYS"
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../Comp4/dataset_kaggle/\"\n",
    "PATH_CKPTS = \"../Comp4/dataset_kaggle/test2_3/\"\n",
    "\n",
    "checkpoint_name = \"test2_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kDGhnFOFfL9"
   },
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了擷取重要的音檔資訊我們取幾個重要的features : \n",
    "(1) melspectrogram : 因為clip的音頻會較為特殊，因此把音檔轉換成頻譜圖，並取它在頻率域上的資訊。\n",
    "(2) spectral centroid : 把頻譜的中心代表聲音的質心，當頻譜中心越小，就代表越多的頻譜能量集中在低頻範圍內。\n",
    "(3) spectral_bandwidth : 計算所對應到的頻譜寬。 \n",
    "(4) spectral rolloff : 比該頻率低的頻率的所有能量大於85%的整個頻率的能量。\n",
    "(5) zero_crossing_rate : 為過零率，音檔的語音信息通過零點的次數。\n",
    "(6) tonnetz : 計算它的音調質心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "feDxPL-gwkYU"
   },
   "outputs": [],
   "source": [
    "def mel(wave_file):\n",
    "    y, sr = librosa.load(wave_file, sr = None)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000) \n",
    "    return spectrogram\n",
    "\n",
    "def centroid(wave_file):\n",
    "    y, sr = librosa.load(wave_file, sr = None)\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    return centroid\n",
    "\n",
    "def bandwidth(wave_file):\n",
    "    y, sr = librosa.load(wave_file, sr = None)\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    return bandwidth\n",
    "\n",
    "def rolloff(wave_file):\n",
    "    y, sr = librosa.load(wave_file, sr = None)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    return rolloff\n",
    "\n",
    "def zcr(wave_file):\n",
    "    y, sr = librosa.load(wave_file, sr = None)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
    "    return zcr\n",
    "\n",
    "def tonnetz(wave_file):\n",
    "    y, sr = librosa.load(wave_file, sr = None)\n",
    "    y = librosa.effects.harmonic(y)\n",
    "    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "    return tonnetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uyIPmf3PwkYn"
   },
   "outputs": [],
   "source": [
    "def get_data(dname):\n",
    "    tmp = pd.read_csv(PATH_TO_DATA + dname)\n",
    "    tmp[\"mix_fname\"] = tmp[\"mix_fname\"].apply(lambda x: x[10:])\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def get_features(ds, idx):\n",
    "    Melspectrogram = mel(PATH_TO_DATA + ds[\"mix_fname\"][idx])\n",
    "    Melspectrogram = (librosa.power_to_db(Melspectrogram,ref = np.max)+40).T/40\n",
    "    Centroid = sklearn.preprocessing.minmax_scale(centroid(PATH_TO_DATA + ds[\"mix_fname\"][idx])[0],axis = 0) \n",
    "    Centroid = np.array([Centroid.tolist()],dtype = np.float32).T\n",
    "    Bandwidth = sklearn.preprocessing.minmax_scale(bandwidth(PATH_TO_DATA + ds[\"mix_fname\"][idx])[0],axis = 0)\n",
    "    Bandwidth = np.array([Bandwidth.tolist()],dtype = np.float32).T\n",
    "    Rolloff = sklearn.preprocessing.minmax_scale(rolloff(PATH_TO_DATA + ds[\"mix_fname\"][idx])[0],axis = 0)\n",
    "    Rolloff = np.array([Rolloff.tolist()],dtype = np.float32).T\n",
    "    Zcr = sklearn.preprocessing.minmax_scale(rolloff(PATH_TO_DATA + ds[\"mix_fname\"][idx])[0],axis = 0)\n",
    "    Zcr = np.array([Zcr.tolist()],dtype = np.float32).T\n",
    "    Tonnetz = tonnetz(PATH_TO_DATA + ds[\"mix_fname\"][idx]).T\n",
    "    return np.concatenate((Melspectrogram, Centroid, Bandwidth, Rolloff, Zcr, Tonnetz),axis = 1)\n",
    "\n",
    "\n",
    "def get_wav_ds(ds):\n",
    "    wav_ds = []\n",
    "    \n",
    "    for i in range(ds.shape[0]):\n",
    "        \n",
    "        wav_ds.append(get_features(ds, i))\n",
    "        \n",
    "    wav_ds = np.asarray(wav_ds)\n",
    "    \n",
    "    return wav_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將response的1、2、3轉換成三維的由01組成的陣列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zzIS-URwkYo"
   },
   "outputs": [],
   "source": [
    "def get_label_ds(ds):\n",
    "    label = []\n",
    "    num = ds[\"n_cues\"].tolist()\n",
    "    for i in range(len(num)):\n",
    "        if num[i]==1:\n",
    "            label.append([1,0,0])\n",
    "        if num[i]==2:\n",
    "            label.append([0,1,0])\n",
    "        if num[i]==3:\n",
    "            label.append([0,0,1])\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分別從train和validation以及要預測的sample取出上面所彙整的features，以及把各個response轉換成我們所要對應的三維陣列形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDomCEmOwkYq"
   },
   "outputs": [],
   "source": [
    "tr = get_data(\"train.csv\")\n",
    "val = get_data(\"val.csv\")\n",
    "te = get_data(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WChzvgFwkYr"
   },
   "outputs": [],
   "source": [
    "tr_label = get_label_ds(tr)\n",
    "val_label = get_label_ds(val)\n",
    "te_label = get_label_ds(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1s9-4DdbFkh-"
   },
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分別將train和validation以及要預測的sample我們需要用到資料轉成tf的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSnZKUrDwkYv"
   },
   "outputs": [],
   "source": [
    "class ds_train_generator(object):\n",
    "    def __init__(self, dataset,label):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.dataset_wav = get_wav_ds(self.dataset)\n",
    "        self.labels = label\n",
    "      \n",
    "    def preprocessing(self, wav_file, label):\n",
    "        wav_file = tf.cast(wav_file, tf.float32)\n",
    "      \n",
    "        return wav_file, label\n",
    "        \n",
    "    def generate(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.dataset_wav, self.labels))\n",
    "        dataset = dataset.map(self.preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8U5BamlwkYx"
   },
   "outputs": [],
   "source": [
    "class ds_val_generator(object):\n",
    "    def __init__(self, dataset,label):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.dataset_wav = get_wav_ds(self.dataset)\n",
    "        self.labels = label\n",
    "\n",
    "    def preprocessing(self, wav_file, label):\n",
    "        wav_file = tf.cast(wav_file, tf.float32)\n",
    "      \n",
    "        return wav_file, label\n",
    "        \n",
    "    def generate(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.dataset_wav, self.labels))\n",
    "        dataset = dataset.map(self.preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUyKns5TwkY0"
   },
   "outputs": [],
   "source": [
    "class ds_te_generator(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.dataset_wav = get_wav_ds(self.dataset)\n",
    "\n",
    "    def preprocessing(self, wav_file):\n",
    "        wav_file = tf.cast(wav_file, tf.float32)\n",
    "      \n",
    "        return wav_file\n",
    "        \n",
    "    def generate(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.dataset_wav))\n",
    "        dataset = dataset.map(self.preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2491inowkY2"
   },
   "outputs": [],
   "source": [
    "train_gen = ds_train_generator(tr,tr_label)\n",
    "val_gen = ds_val_generator(val,val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2462,
     "status": "error",
     "timestamp": 1578840634518,
     "user": {
      "displayName": "Yang Lo",
      "photoUrl": "",
      "userId": "07530317156869475190"
     },
     "user_tz": -480
    },
    "id": "0QlXkTNIwkY4",
    "outputId": "1f83900d-d0ec-415e-b0b5-332b4a79e18f"
   },
   "outputs": [],
   "source": [
    "ds_train = train_gen.generate()\n",
    "ds_val = val_gen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXKWmXTocNN3"
   },
   "outputs": [],
   "source": [
    "te_gen = ds_te_generator(te)\n",
    "ds_te = te_gen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLHeqrI-wkY7"
   },
   "outputs": [],
   "source": [
    "ds_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8U1hY16FovO"
   },
   "source": [
    "# Training net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡都是用keras裡的1D Convolutional Neural Networks去疊我們所需要的架構，它比較適用於time series的data或特定訊號在固定的時間區間中，而我們padding的方式為valid的方式，代表它輸出的維度會低於它輸入的維度，而我們activation的function取的是relu的方式，它表現比較好的原因為因為我們這裡取的音檔資訊，會有很多高低起伏的資訊，這裡我們用relu的特性all or none law，若音檔的振幅未達一定高度，可以合理懷疑它是不重要的聲音資訊，輸出就會為0，最後我們用soft.max使它輸出為0到1的數值，我們會挑選在三個中數值最大的為它的預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLgIXHQnj3Jd"
   },
   "outputs": [],
   "source": [
    "class net(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.net0_conv1 = tf.keras.layers.Conv1D(512, 9, padding='same')\n",
    "        self.net0_conv2 = tf.keras.layers.Conv1D(512, 9, padding='same')\n",
    "        self.net1_conv1 = tf.keras.layers.Conv1D(256, 6, padding='same')\n",
    "        self.net1_conv2 = tf.keras.layers.Conv1D(256, 6, padding='same')\n",
    "        self.net2_conv1 = tf.keras.layers.Conv1D(128, 3, padding='same')\n",
    "        self.net2_conv2 = tf.keras.layers.Conv1D(128, 3, padding='same')\n",
    "        #self.net3_conv1 = tf.keras.layers.Conv1D(128, 2,padding='valid', activation='relu')\n",
    "        #self.net3_conv2 = tf.keras.layers.Conv1D(64, 2,padding='valid', activation='relu')\n",
    "        \n",
    "        self.net0_mp = tf.keras.layers.MaxPool1D(3)\n",
    "        self.net1_mp = tf.keras.layers.MaxPool1D(3)\n",
    "        self.net2_mp = tf.keras.layers.MaxPool1D(3)\n",
    "        #self.net3_mp = tf.keras.layers.MaxPool1D(3)\n",
    "        \n",
    "        self.net1_do = tf.keras.layers.Dropout(0.3)\n",
    "        self.net2_do = tf.keras.layers.Dropout(0.3)\n",
    "        self.net4_do = tf.keras.layers.Dropout(0.5)\n",
    "        self.net5_do = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "        self.net4_flat = tf.keras.layers.Flatten()\n",
    "        self.net4_den = tf.keras.layers.Dense(1024)\n",
    "        self.net5_den = tf.keras.layers.Dense(256)\n",
    "        self.net6_den = tf.keras.layers.Dense(3)\n",
    "    \n",
    "    def call(self, inputs):             \n",
    "        net0 = self.net0_conv1(inputs)\n",
    "        net0 = tf.nn.leaky_relu(net0)\n",
    "        net0 = self.net0_conv2(net0)\n",
    "        net0 = tf.nn.leaky_relu(net0)\n",
    "        net0 = self.net0_mp(net0)\n",
    "        \n",
    "        net1 = self.net1_conv1(net0)\n",
    "        net1 = tf.nn.leaky_relu(net1)\n",
    "        net1 = self.net1_conv2(net1)\n",
    "        net1 = tf.nn.leaky_relu(net1)\n",
    "        net1 = self.net1_do(net1)\n",
    "        net1 = self.net1_mp(net1)\n",
    "        \n",
    "        net2 = self.net2_conv1(net1)\n",
    "        net2 = tf.nn.leaky_relu(net2)\n",
    "        net2 = self.net2_conv2(net2)\n",
    "        net2 = tf.nn.leaky_relu(net2)\n",
    "        net2 = self.net2_do(net2)\n",
    "        net2 = self.net2_mp(net2)\n",
    "        \n",
    "        #net3 = self.net3_conv1(net2)\n",
    "        #net3 = self.net3_conv2(net3)\n",
    "        #net3 = self.net3_mp(net3)\n",
    "        \n",
    "        net4 = self.net4_flat(net2)\n",
    "        net4 = self.net4_den(net4)\n",
    "        net4 = tf.nn.leaky_relu(net4)\n",
    "        net4 = self.net4_do(net4)\n",
    "        \n",
    "        net5 = self.net5_den(net4)\n",
    "        net5 = tf.nn.leaky_relu(net5)\n",
    "        net5 = self.net5_do(net5)\n",
    "        \n",
    "        net6 = self.net6_den(net5)\n",
    "        outputs = tf.nn.sigmoid(net6)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為我們是考慮多類別的分類問題，因此在損失函數上也是考慮多類別的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PR8hTzyAwkY-"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "def total_loss(true_answer,pred_answer):\n",
    "    true_answer = tf.cast(true_answer,dtype = tf.float32)\n",
    "    return cross_entropy(true_answer, pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQvBsO6FwkZA"
   },
   "outputs": [],
   "source": [
    "net = net()\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1578764181212,
     "user": {
      "displayName": "Yang Lo",
      "photoUrl": "",
      "userId": "07530317156869475190"
     },
     "user_tz": -480
    },
    "id": "CL8uGjx8wkZB",
    "outputId": "e15b4f87-3b82-47f9-adb3-2fd24e30aa4c"
   },
   "outputs": [],
   "source": [
    "last_ckp = tf.train.latest_checkpoint(PATH_CKPTS)\n",
    "start_epoch = 0\n",
    "\n",
    "if last_ckp:\n",
    "    ckpt = tf.train.Checkpoint(optimizer=optimizer, net=net)\n",
    "    ckpt.restore(last_ckp)\n",
    "    start_epoch = int(last_ckp.split(\"-\")[-1])\n",
    "\n",
    "print(f'Resume training from epoch {start_epoch}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuYRhKEmwkZD"
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(optimizer=optimizer, net=net)\n",
    "\n",
    "manager = tf.train.CheckpointManager(ckpt, PATH_CKPTS, max_to_keep=EPOCHS,\n",
    "                                     checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFqcWq2XwkZE"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(audio, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_label = net(audio)        \n",
    "        # loss\n",
    "        loss = total_loss(label, pred_label)\n",
    "    \n",
    "    grads = tape.gradient(loss, net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zyxfe-j_wkZG"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(audio):\n",
    "    pred_label = net(audio)\n",
    "    \n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZLNL-NFwkZI"
   },
   "outputs": [],
   "source": [
    "def val(dataset):\n",
    "    out = []\n",
    "\n",
    "    for audio,label in dataset:\n",
    "        curr = test_step(audio)\n",
    "        out.extend(curr.numpy().tolist())\n",
    "    out = np.asarray(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def cut_thres(y_pred):\n",
    "    pred = []\n",
    "    for i in range(len(y_pred)):\n",
    "        pred.append(np.where(y_pred[i]==np.max(y_pred[i]),1,0))\n",
    "    pred = np.asarray(pred)\n",
    "    return pred\n",
    "\n",
    "def cal_acc(y_true, y_pred):\n",
    "    output = []\n",
    "    for i in range(len(y_pred)):\n",
    "        is_same = (y_true[i] == y_pred[i].tolist())\n",
    "        output.append(int(is_same))\n",
    "        \n",
    "    return sum(output)/len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxU2JWq5FuAt"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 207198,
     "status": "ok",
     "timestamp": 1578764394612,
     "user": {
      "displayName": "Yang Lo",
      "photoUrl": "",
      "userId": "07530317156869475190"
     },
     "user_tz": -480
    },
    "id": "olCaJ1W0wkZK",
    "outputId": "4475a2d2-6af4-474b-9d8c-7f1a1d5b9c34"
   },
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "val_acc = []\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    epoch = i + 1\n",
    "    t_loss = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for audio, label in ds_train:\n",
    "        tmp_loss = train_step(audio, label)\n",
    "        t_loss.append(tmp_loss)\n",
    "        \n",
    "    print(\"\\nLoss: {}\".format(np.mean(t_loss)))\n",
    "    \n",
    "    val_pred = val(ds_val)\n",
    "    pred = cut_thres(val_pred)\n",
    "    val_accuracy = cal_acc(val_label, pred)\n",
    "    val_acc.append(val_accuracy)\n",
    "    print(\"Validation Acc is {:.4f} \".format(val_accuracy))\n",
    "    \n",
    "    print('Time for epoch {} is {:.4f} sec'.format(epoch, time.time()-start))\n",
    "    \n",
    "    manager.save(checkpoint_number=epoch)\n",
    "    loss_hist.append(np.mean(t_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TC0213Fe-DZZ"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szO4B9nEwkZL"
   },
   "outputs": [],
   "source": [
    "index = np.array(tf.math.argmax(val_acc)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1578764398988,
     "user": {
      "displayName": "Yang Lo",
      "photoUrl": "",
      "userId": "07530317156869475190"
     },
     "user_tz": -480
    },
    "id": "Hq5B_ZsQBxoZ",
    "outputId": "fc82690e-172e-4644-be6c-caa0eaa6e8b3"
   },
   "outputs": [],
   "source": [
    "val_acc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1324,
     "status": "ok",
     "timestamp": 1578764402041,
     "user": {
      "displayName": "Yang Lo",
      "photoUrl": "",
      "userId": "07530317156869475190"
     },
     "user_tz": -480
    },
    "id": "8ERDmW7C-MNX",
    "outputId": "8de186d4-bed8-4856-9a07-52ef6f266975"
   },
   "outputs": [],
   "source": [
    "last_ckp =  PATH_CKPTS + \"test2_3-124\" # PATH_CKPTS + checkpoint_name + '-' + str(index+1)\n",
    "start_epoch = 0\n",
    "\n",
    "if last_ckp:\n",
    "    ckpt = tf.train.Checkpoint(optimizer=optimizer, net=net)\n",
    "    ckpt.restore(last_ckp)\n",
    "    start_epoch = int(last_ckp.split(\"-\")[-1])\n",
    "\n",
    "print(f'Resume training from epoch {start_epoch}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zY8O0taH_1Wu"
   },
   "outputs": [],
   "source": [
    "def testing(dataset):\n",
    "    out = []\n",
    "\n",
    "    for audio in dataset:\n",
    "        curr = test_step(audio)\n",
    "        out.extend(curr.numpy().tolist())\n",
    "    out = np.asarray(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def pred_value(y_pred):\n",
    "    pred = []\n",
    "    for i in range(len(y_pred)):\n",
    "        pred.append(np.where(y_pred[i]==np.max(y_pred[i]))[0][0]+1)\n",
    "    pred = np.asarray(pred)\n",
    "    return pred\n",
    "\n",
    "def output(pred):    \n",
    "    output = pd.DataFrame({\"id\": range(len(pred)),\n",
    "                           \"n_cues\": pred})\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y73igB8c9xFC"
   },
   "outputs": [],
   "source": [
    "te_pred = testing(ds_te)\n",
    "predict = pred_value(te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "En13no-S9m5I"
   },
   "outputs": [],
   "source": [
    "output(predict).to_csv(PATH_TO_DATA + \"test2_3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "audio3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
