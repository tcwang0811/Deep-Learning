{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度學習第二次競賽報告 - 第12組 工具人智慧\n",
    "組員：\n",
    "<br/>107024501 高瑀鍹\n",
    "<br/>107024506 王子誠\n",
    "<br/>107024511 羅揚\n",
    "<br/>107024522 戴子翔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlines\n",
    "這次的競賽中，我們主要的方法條列如下：\n",
    "1. Balance Data\n",
    "2. Data Augmentation\n",
    "3. Models\n",
    "4. Training\n",
    "5. Non-max Suppression\n",
    "6. 水平翻轉預測\n",
    "7. Ensemble\n",
    "\n",
    "以下會個別介紹每個方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Install TensorFlow\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "\n",
    "from shutil import copyfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_name =  [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \n",
    "                 \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \n",
    "                 \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \n",
    "                 \"sheep\", \"sofa\", \"train\",\"tvmonitor\"]\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common params\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 20\n",
    "MAX_OBJECTS_PER_IMAGE = 20\n",
    "\n",
    "# dataset params\n",
    "PATH_TO_DATA = \"../data\"\n",
    "DATA_PATH = PATH_TO_DATA + '/train_1.txt'\n",
    "IMAGE_DIR = PATH_TO_DATA + '/VOCdevkit_train/VOCdevkit_train/VOC2007/JPEGImages/'\n",
    "\n",
    "# model params\n",
    "CELL_SIZE = 7\n",
    "BOXES_PER_CELL = 2\n",
    "OBJECT_SCALE = 1\n",
    "NOOBJECT_SCALE = 0.5\n",
    "CLASS_SCALE = 1\n",
    "COORD_SCALE = 5\n",
    "\n",
    "# training params\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 15\n",
    "\n",
    "is_fine_tune = False\n",
    "\n",
    "CKT_Dir = \"../ckpts/17_resnet_balance\"\n",
    "checkpoint_name = 'yolo_v1_17_resnet_balance'\n",
    "\n",
    "is_load_best = False\n",
    "\n",
    "PROB_THRES = 0.01\n",
    "IOU_THRES = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同label之間的個數相差很多，例如 : 人(label 14)的個數就高達5392筆，相反地公車(label 5)的個數就只有272筆，最高類別的個數與最低類別的個數相差約20倍，這會造成預測結果被混淆而傾向將多數的結果視為人，且公車因為資料少，就很難判定。\n",
    "<br/>我們藉由不增加原先個數多的label設定條件，並增加原始label個數少的物品，這樣就可以達到他們不同類別個數之間相差沒有那麼多。根據最後的結果，平衡後的資料最高類別的個數 (6979筆; label 8) 與最低類別的個數 (3241筆; label 0) 相差約2倍，可見類別的個數相較之前平衡許多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def list_add(a,b):\n",
    "    c = []\n",
    "    for i in range(len(a)):\n",
    "        c.append(a[i]+b[i])\n",
    "    return c\n",
    "\n",
    "training_data_file = open(\"../data/pascal_voc_training_data.txt\", \"r\")\n",
    "class imbalance:\n",
    "  def __init__(self):\n",
    "    self.each_class_count = []\n",
    "    self.each_pic_count = []\n",
    "    self.add_pic_index = []\n",
    "    self.new = []\n",
    "\n",
    "    for i, line in enumerate(training_data_file):\n",
    "      line = line.strip()\n",
    "      self.new.append(line)\n",
    "      a = line.split()\n",
    "      b = len(a)\n",
    "      e = []\n",
    "      for j in range(5,b,5):\n",
    "        e.append(a[j])\n",
    "        f = [e.count('0'),e.count('1'),e.count('2'),e.count('3'),e.count('4'),e.count('5'),e.count('6'),e.count('7'),e.count('8'),e.count('9'),e.count('10'),e.count('11'),e.count('12'),e.count('13'),e.count('14'),e.count('15'),e.count('16'),e.count('17'),e.count('18'),e.count('19')]\n",
    "      self.each_pic_count.append(f)\n",
    "    g = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(4974):\n",
    "      g = list_add(g,self.each_pic_count[i])\n",
    "    self.each_class_count = g \n",
    "    d = self.each_pic_count\n",
    "    \n",
    "    #index \n",
    "    index5 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][5]>0) and (d[i][14] == 0):\n",
    "        index5.append(i)\n",
    "    index10 = []\n",
    "    for i in range(4974):  \n",
    "      if (d[i][10]>0) and (d[i][14] < 2) and (d[i][8] < 3):\n",
    "        index10.append(i)\n",
    "    index0 = []\n",
    "    for i in range(4974): \n",
    "      if (d[i][0]>0) and (d[i][14] == 0) and (d[i][5] == 0)and (d[i][6] == 0) and(d[i][8] == 0) :\n",
    "        index0.append(i)\n",
    "    index18 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][18]>0) and (d[i][14] == 0) and (d[i][5] == 0)and (d[i][6] == 0) and(d[i][8] == 0) and (d[i][10] == 0) :\n",
    "        index18.append(i)\n",
    "    index19 = []\n",
    "    for i in range(4974):    \n",
    "      if (d[i][19]>0) and (d[i][14] == 0) and (d[i][8] == 0) and (d[i][10] == 0) and (d[i][0] == 0) and (d[i][18] == 0):\n",
    "        index19.append(i)\n",
    "    index9 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][9]>0) and (d[i][14] == 0):\n",
    "        index9.append(i)\n",
    "    index1 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][1]>0) and (d[i][14] == 0) :\n",
    "        index1.append(i)\n",
    "    index2 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][2]>0) and (d[i][14] == 0):\n",
    "        index2.append(i)\n",
    "    index3 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][3]>0) and (d[i][2]==0) and(d[i][14] == 0) :\n",
    "        index3.append(i)\n",
    "    index7 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][7]>0) and(d[i][14] == 0) :\n",
    "        index7.append(i)\n",
    "    index11 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][11]>0) and(d[i][14] == 0) :\n",
    "        index11.append(i)\n",
    "    index12 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][12]>0) and(d[i][14] == 0) :\n",
    "        index12.append(i)\n",
    "    index13 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][13]>0) and (d[i][14] == 0) and (d[i][6] == 0) and (d[i][10] == 0):\n",
    "        index13.append(i)\n",
    "    index16 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][16]>0) and (d[i][14] == 0) :\n",
    "        index16.append(i)\n",
    "    index17 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][17]>0) and (d[i][14] == 0) and (d[i][15] == 0)and (d[i][8] == 0) :\n",
    "        index17.append(i)\n",
    "    index4 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][4]>0) and(d[i][14] == 0) :\n",
    "        index4.append(i)\n",
    "    index15 = []\n",
    "    for i in range(4974):\n",
    "      if (d[i][15]>0) and(d[i][14] == 0) :\n",
    "        index15.append(i)\n",
    "    self.add_pic_index = list(np.repeat(index5,26))+list(np.repeat(index10,60))+list(np.repeat(index0,10))+list(np.repeat(index18,15))+list(np.repeat(index19,20))+list(np.repeat(index9,15))+list(np.repeat(index1,40))+list(np.repeat(index2,6))+list(np.repeat(index3,10))+list(np.repeat(index7,10))+list(np.repeat(index11,10))+list(np.repeat(index12,40))+list(np.repeat(index13,40))+list(np.repeat(index16,10))+list(np.repeat(index17,10))+list(np.repeat(index4,6))+list(np.repeat(index15,3))\n",
    "    for i in self.add_pic_index:\n",
    "      self.new.append(self.new[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = imbalance()\n",
    "\n",
    "num = []\n",
    "for line in a.new:\n",
    "  line = line.strip()\n",
    "  temp = line.split()\n",
    "  for j in range(5,len(temp),5):\n",
    "    num.append(temp[j])\n",
    "result = Counter(num)\n",
    "\n",
    "new_data = a.new\n",
    "\n",
    "with open('../data/train_1.txt', 'w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在做Data augmentation時，我們嘗試了兩種方法：\n",
    "1. 參考Paper中提到的augmentation方法，包含Sacling、Translation、調整圖片的飽和度、曝光度等，此外，我們也對圖片進行水平與垂直翻轉。我們以20%的機率保留原圖，確保模型可以看到部分的原始圖片。\n",
    "2. 我們也嘗試自己設計了augmentation方法，使用到的方法包含Sacling、Translation、Rotation、Crop、Shear，以及調整對比、模糊、Random noise、亮度、飽和度、曝光度，以及轉為灰階圖、RGB shuffle、水平與垂直翻轉。因為使用的augmentation方法更多，我們選擇保留40%的原圖。\n",
    "\n",
    "因為tensorflow中並沒有提供我們足夠多的data augmentation方法，而且進行data augmentation時，我們也必須將bounding box進行調整。所以，我們使用imgaug package進行data augmentation，起初我們擔心使用非tensorflow會降低效率，但根據tensorflow的官網，準備dataset的階段是使用CPU進行運算，因此應該不會降低太多效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_flip(image,xcenter,ycenter):\n",
    "    up_down_outcome = tf.random.uniform([1],0,1)\n",
    "    right_left_outcome = tf.random.uniform([1],0,1)\n",
    "\n",
    "    x_0 = tf.not_equal(xcenter,0.)\n",
    "    y_0 = tf.not_equal(ycenter,0.)\n",
    "    grand = tf.cast(tf.where(tf.math.logical_or(x_0,y_0),IMAGE_SIZE,0),\n",
    "                    tf.float32)\n",
    "\n",
    "    if up_down_outcome<up_down_flip_p:\n",
    "        image = tf.image.flip_up_down(image)\n",
    "        ycenter = grand-ycenter\n",
    "\n",
    "    if right_left_outcome<left_right_flip_p:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        xcenter = grand-xcenter\n",
    "\n",
    "    return image, xcenter, ycenter\n",
    "\n",
    "@tf.function\n",
    "def to_gray(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "\n",
    "    if prob<gray_p:\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def gaussian_noise(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "    \n",
    "    if prob<noise_p:\n",
    "        noise = tf.random.normal(image.shape,stddev=5)\n",
    "        image = tf.math.add(image, noise)\n",
    "        \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def brightness(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "    \n",
    "    if prob<brightness_p:\n",
    "        image = tf.image.random_brightness(image,5)\n",
    "        \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def hue(image):\n",
    "    prob = tf.random.uniform([1], 0, 1)\n",
    "    \n",
    "    if prob < hue_p:\n",
    "        image = tf.image.random_hue(image, 0.5)\n",
    "        \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def saturation(image):\n",
    "    prob = tf.random.uniform([1], 0, 1)\n",
    "    \n",
    "    if prob < saturation_p:\n",
    "        image = tf.image.random_saturation(image, 0, 1.5)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Paper Augmentation\n",
    "以下為實作Paper的data augmentation的程式碼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augumentation parameter\n",
    "up_down_flip_p = 0.1\n",
    "left_right_flip_p = 0.5\n",
    "\n",
    "theSame = 0.1\n",
    "crop_p = 0\n",
    "geom_p = 0.8\n",
    "gray_p = 0\n",
    "noise_p = 0\n",
    "hue_p = 0.8\n",
    "saturation_p = 0.8\n",
    "brightness_p = 0\n",
    "contrast_p = 0\n",
    "blur_p = 0\n",
    "\n",
    "rotate_range=(-45, 45)\n",
    "scale_range=(0.8, 1.2)\n",
    "translate_range=(-0.2, 0.2)\n",
    "shear_range=(-20, 20)\n",
    "crop_pad_range=(-0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgaug_trans(image,labels):\n",
    "    n = tf.math.count_nonzero(labels[:,0]).numpy()\n",
    "    image = image.numpy()\n",
    "    labels = labels.numpy()\n",
    "    output = np.zeros_like(labels)\n",
    "\n",
    "    center_x = labels[:,0]\n",
    "    center_y = labels[:,1]\n",
    "    w_half = labels[:,2] / 2\n",
    "    h_half = labels[:,3] / 2\n",
    "\n",
    "    tempbb = [BoundingBox(x1=center_x[i] - w_half[i], y1=center_y[i] - h_half[i],\n",
    "                          x2=center_x[i] + w_half[i], y2=center_y[i] + h_half[i]) for i in range(n)]\n",
    "    bbs = BoundingBoxesOnImage(tempbb, shape=image.shape)\n",
    "\n",
    "    seq = iaa.Sequential(\n",
    "      [\n",
    "           iaa.Sometimes(geom_p,\n",
    "                     iaa.SomeOf((1, 3), [\n",
    "                                       iaa.Affine(translate_percent={\"x\":(translate_range[0], translate_range[1])},mode =ia.ALL),\n",
    "                                       iaa.Affine(translate_percent={\"y\":(translate_range[0], translate_range[1])},mode = ia.ALL),\n",
    "                                       iaa.Affine(scale=(scale_range[0], scale_range[1]),mode = ia.ALL)\n",
    "                                       ],\n",
    "                                random_order=True)\n",
    "                     )\n",
    "    ])\n",
    "\n",
    "    image_aug, bbs_aug = seq(image=image, bounding_boxes=bbs)\n",
    "\n",
    "    if len(bbs_aug.remove_out_of_image().bounding_boxes)==n:\n",
    "        theIdx = [i for i in range(n)]\n",
    "    else:\n",
    "        set_bb = set(bbs_aug.remove_out_of_image().bounding_boxes)\n",
    "        theIdx = [i for i in range(n) if bbs_aug.bounding_boxes[i] in set_bb]\n",
    "\n",
    "    selected_labels = labels[theIdx, 4]\n",
    "    clip_bbs = bbs_aug.remove_out_of_image().clip_out_of_image().bounding_boxes\n",
    "    for i in range(len(theIdx)):\n",
    "        theBox = clip_bbs[i]\n",
    "        output[i, 0] = (theBox.x1 + theBox.x2) / 2 # x center\n",
    "        output[i, 1] = (theBox.y1 + theBox.y2) / 2 # y center\n",
    "        output[i, 2] = (theBox.x2 - theBox.x1) # w\n",
    "        output[i, 3] = (theBox.y2 - theBox.y1) # h\n",
    "        output[i, 4] = selected_labels[i]\n",
    "\n",
    "    return image_aug, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def data_aug(image,labels):\n",
    "    same_sample = tf.random.uniform([1],0,1)\n",
    "    if same_sample > theSame:\n",
    "        # not the same\n",
    "        image, labels = tf.py_function(\n",
    "            func=imgaug_trans,\n",
    "            inp=[image,labels],\n",
    "            Tout=[tf.float32,tf.float32]\n",
    "            )\n",
    "        image = hue(image)\n",
    "        image = saturation(image)\n",
    "\n",
    "    return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Our Augmentation\n",
    "以下是實作我們自己設計的augmentation的程式碼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augumentation parameter\n",
    "up_down_flip_p = 0.05\n",
    "left_right_flip_p = 0.5\n",
    "\n",
    "theSame = 0.4\n",
    "crop_p = 0.6\n",
    "geom_p = 0.8\n",
    "brightness_p = 0.3\n",
    "hue_p = 0.8\n",
    "saturation_p = 0.8\n",
    "contrast = 0.3\n",
    "gray_p = 0.1\n",
    "noise_p = 0.4\n",
    "blur = 0.2\n",
    "\n",
    "rotate_range=(-45, 45)\n",
    "scale_range=(0.2, 1.2)\n",
    "translate_range=(-0.2, 0.2)\n",
    "shear_range=(-20, 20)\n",
    "crop_pad_range=(-0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgaug_trans(image,labels):\n",
    "    n = tf.math.count_nonzero(labels[:,0]).numpy()\n",
    "    image = image.numpy()\n",
    "    labels = labels.numpy()\n",
    "    output = np.zeros_like(labels)\n",
    "\n",
    "    center_x = labels[:,0]\n",
    "    center_y = labels[:,1]\n",
    "    w_half = labels[:,2] / 2\n",
    "    h_half = labels[:,3] / 2\n",
    "\n",
    "    tempbb = [BoundingBox(x1=center_x[i] - w_half[i], y1=center_y[i] - h_half[i],\n",
    "                          x2=center_x[i] + w_half[i], y2=center_y[i] + h_half[i]) for i in range(n)]\n",
    "    bbs = BoundingBoxesOnImage(tempbb, shape=image.shape)\n",
    "\n",
    "    seq = iaa.Sequential(\n",
    "      [\n",
    "       iaa.Sometimes(crop_p,iaa.CropAndPad(percent=(crop_pad_range[0], crop_pad_range[1]))),\n",
    "       iaa.Sometimes(geom_p,\n",
    "                     iaa.SomeOf((1, 5),[\n",
    "                                       iaa.Affine(translate_percent={\"x\":(translate_range[0], translate_range[1])}),\n",
    "                                       iaa.Affine(translate_percent={\"y\":(translate_range[0], translate_range[1])}),\n",
    "                                       iaa.Affine(scale=(scale_range[0], scale_range[1])),\n",
    "                                       iaa.Affine(rotate=(rotate_range[0], rotate_range[1])),\n",
    "                                       iaa.Affine(shear=(shear_range[0],shear_range[1]))\n",
    "                                       ],\n",
    "                                random_order=True)\n",
    "                     ),\n",
    "       iaa.Sometimes(contrast,\n",
    "                     iaa.OneOf([\n",
    "                                iaa.contrast.LinearContrast(alpha=(1.25, 1.5),per_channel=True),\n",
    "                                iaa.contrast.LinearContrast(alpha=(0.25, 0.5),per_channel=True),\n",
    "                                iaa.contrast.LinearContrast(alpha=(0.25, 0.5)),\n",
    "                                iaa.contrast.LinearContrast(alpha=(1.25, 1.5)),\n",
    "                                iaa.ChannelShuffle()\n",
    "                                ]\n",
    "                               )\n",
    "                     ),\n",
    "       iaa.Sometimes(blur, iaa.GaussianBlur(sigma=(0.1,3)))\n",
    "    ])\n",
    "\n",
    "    image_aug, bbs_aug = seq(image=image, bounding_boxes=bbs)\n",
    "\n",
    "    if len(bbs_aug.remove_out_of_image().bounding_boxes)==n:\n",
    "        theIdx = [i for i in range(n)]\n",
    "    else:\n",
    "        set_bb = set(bbs_aug.remove_out_of_image().bounding_boxes)\n",
    "        theIdx = [i for i in range(n) if bbs_aug.bounding_boxes[i] in set_bb]\n",
    "\n",
    "    selected_labels = labels[theIdx, 4]\n",
    "    clip_bbs = bbs_aug.remove_out_of_image().clip_out_of_image().bounding_boxes\n",
    "    for i in range(len(theIdx)):\n",
    "        theBox = clip_bbs[i]\n",
    "        output[i, 0] = (theBox.x1 + theBox.x2) / 2 # x center\n",
    "        output[i, 1] = (theBox.y1 + theBox.y2) / 2 # y center\n",
    "        output[i, 2] = (theBox.x2 - theBox.x1) # w\n",
    "        output[i, 3] = (theBox.y2 - theBox.y1) # h\n",
    "        output[i, 4] = selected_labels[i]\n",
    "\n",
    "    return image_aug, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def data_aug(image,labels):\n",
    "    same_sample = tf.random.uniform([1],0,1)\n",
    "    if same_sample > theSame:\n",
    "        # not the same\n",
    "        image = gaussian_noise(image)\n",
    "        image, labels = tf.py_function(\n",
    "            func=imgaug_trans,\n",
    "            inp=[image, labels],\n",
    "            Tout=[tf.float32, tf.float32]\n",
    "            )\n",
    "        image = brightness(image)\n",
    "        image = hue(image)\n",
    "        image = saturation(image)\n",
    "        image = to_gray(image)\n",
    "\n",
    "    return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們嘗試過許多種不同的Pre-trained，包含VGG16、VGG19、Inception V3、NASNet等不同的Pretrained model，我們也嘗試過在Pretrained model後接上許多不同layers。\n",
    "<br/>我們一開始使用VGG16替代Paper的前20層，並在完成augmentation階段後才開始嘗試不同的模型。有些模型的效果一直都很不好，如Inception V3，但有些模型卻是在部分augmentation效果好、其他效果不好，如NASNet。最後嘗試出來表現較好並在不同的augmentation上都能夠有好結果的模型如下：\n",
    "1. 使用ResNet 152替代Paper的前20層，接下來的四層 Convolution+Leaky ReLU 依照Paper的設定，但不使用Padding，最後再接上4096個neuron的全連接層以及Output layer。\n",
    "2. 使用Xception，並在之後使用兩層Convolution+Max pooling+Batch normalization，全連接層則使用一層4096個neuron、一層512個neuron並各自加上Dropout，最後才接上Output layer。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ResNet 152\n",
    "這個模型是上述提到的第一個模型，因為ResNet是使用Caffe訓練，必須要將資料進行前處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\"\n",
    "    Load pascalVOC 2007 dataset and creates an input pipeline ready to be fed into a model.\n",
    "    - Reshapes images into 448 x 448\n",
    "    - converts [0 1] to [-1 1]\n",
    "    - shuffles the input\n",
    "    - builds batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_names = []\n",
    "        self.record_list = []\n",
    "        self.object_num_list = []\n",
    "        # filling the record_list\n",
    "        input_file = open(DATA_PATH, 'r')\n",
    "\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            ss = line.split(' ')\n",
    "            \n",
    "            self.image_names.append(ss[0])\n",
    "            self.record_list.append([float(num) for num in ss[1:]])\n",
    "            self.object_num_list.append(min(len(self.record_list[-1])//5, MAX_OBJECTS_PER_IMAGE))\n",
    "            \n",
    "            # resize newest data\n",
    "            if len(self.record_list[-1]) < MAX_OBJECTS_PER_IMAGE*5:\n",
    "                self.record_list[-1] = self.record_list[-1] +\\\n",
    "                [0., 0., 0., 0., 0.]*\\\n",
    "                (MAX_OBJECTS_PER_IMAGE-len(self.record_list[-1])//5)\n",
    "            elif len(self.record_list[-1]) > MAX_OBJECTS_PER_IMAGE*5:\n",
    "                self.record_list[-1] = self.record_list[-1][:MAX_OBJECTS_PER_IMAGE*5]\n",
    "                \n",
    "        ## shuffle\n",
    "        idx = random.sample(range(len(self.image_names)), len(self.image_names))\n",
    "        self.image_names = [self.image_names[i] for i in idx]\n",
    "        self.record_list = [self.record_list[i] for i in idx]\n",
    "        self.object_num_list = [self.object_num_list[i] for i in idx]\n",
    "\n",
    "    def _data_preprocess(self, image_name, raw_labels, object_num):\n",
    "        image_file = tf.io.read_file(IMAGE_DIR+image_name)\n",
    "        image = tf.io.decode_jpeg(image_file, channels=3)\n",
    "\n",
    "        h = tf.shape(image)[0]\n",
    "        w = tf.shape(image)[1]\n",
    "\n",
    "        width_rate = IMAGE_SIZE * 1.0 / tf.cast(w, tf.float32) \n",
    "        height_rate = IMAGE_SIZE * 1.0 / tf.cast(h, tf.float32) \n",
    "\n",
    "        image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "\n",
    "        raw_labels = tf.cast(tf.reshape(raw_labels, [-1, 5]), tf.float32)\n",
    "\n",
    "        xmin = raw_labels[:, 0]\n",
    "        ymin = raw_labels[:, 1]\n",
    "        xmax = raw_labels[:, 2]\n",
    "        ymax = raw_labels[:, 3]\n",
    "        class_num = raw_labels[:, 4]\n",
    "\n",
    "        xcenter = (xmin + xmax) * 1.0 / 2.0 * width_rate\n",
    "        ycenter = (ymin + ymax) * 1.0 / 2.0 * height_rate\n",
    "\n",
    "        box_w = (xmax - xmin) * width_rate\n",
    "        box_h = (ymax - ymin) * height_rate\n",
    "\n",
    "        image, xcenter, ycenter = random_flip(image, xcenter, ycenter)\n",
    "\n",
    "        labels = tf.stack([xcenter, ycenter, box_w, box_h, class_num], axis = 1)\n",
    "\n",
    "        image, labels = data_aug(image, labels)\n",
    "        image = tf.keras.applications.resnet.preprocess_input(image)\n",
    "\n",
    "        return image, labels, tf.cast(object_num, tf.int32)\n",
    "\n",
    "    def generate(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.image_names, \n",
    "                                                      np.array(self.record_list), \n",
    "                                                      np.array(self.object_num_list)))\n",
    "        dataset = dataset.map(self._data_preprocess, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(1000)\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "        \n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_leaky_relu(inputs, filters, size, stride):\n",
    "    x = layers.Conv2D(filters, size, stride)(inputs)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_fine_tune:\n",
    "    pre_trained_model = tf.keras.applications.ResNet152(include_top=False,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    pre_trained_model.trainable=True\n",
    "\n",
    "    for layer in pre_trained_model.layers:\n",
    "        if not layer.name.startswith(\"conv5_block3_\") and not layer.name.startswith(\"conv5_block2_\"):\n",
    "            layer.trainable = False\n",
    "else:\n",
    "    pre_trained_model = tf.keras.applications.ResNet152(include_top=False,\n",
    "                                                        weights='imagenet',\n",
    "                                                        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    pre_trained_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pre_trained_model.output\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 2)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4096, kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.01))(x)\n",
    "x = layers.LeakyReLU(0.1)(x)\n",
    "outputs = layers.Dense(CELL_SIZE*CELL_SIZE*(BOXES_PER_CELL*5+20),\n",
    "                       activation='relu', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.01))(x)\n",
    "\n",
    "YOLO = keras.Model(inputs=pre_trained_model.input, outputs=outputs, name=\"YOLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Xception\n",
    "這個模型是上述提到的第二個模型，因為Xception是使用tensorflow訓練，必須要將資料進行前處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\"\n",
    "    Load pascalVOC 2007 dataset and creates an input pipeline ready to be fed into a model.\n",
    "    - Reshapes images into 448 x 448\n",
    "    - converts [0 1] to [-1 1]\n",
    "    - shuffles the input\n",
    "    - builds batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_names = []\n",
    "        self.record_list = []\n",
    "        self.object_num_list = []\n",
    "        # filling the record_list\n",
    "        input_file = open(DATA_PATH, 'r')\n",
    "\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            ss = line.split(' ')\n",
    "            \n",
    "            self.image_names.append(ss[0])\n",
    "            self.record_list.append([float(num) for num in ss[1:]])\n",
    "            self.object_num_list.append(min(len(self.record_list[-1])//5, MAX_OBJECTS_PER_IMAGE))\n",
    "            \n",
    "            # resize newest data\n",
    "            if len(self.record_list[-1]) < MAX_OBJECTS_PER_IMAGE*5:\n",
    "                self.record_list[-1] = self.record_list[-1] +\\\n",
    "                [0., 0., 0., 0., 0.]*\\\n",
    "                (MAX_OBJECTS_PER_IMAGE-len(self.record_list[-1])//5)\n",
    "            elif len(self.record_list[-1]) > MAX_OBJECTS_PER_IMAGE*5:\n",
    "                self.record_list[-1] = self.record_list[-1][:MAX_OBJECTS_PER_IMAGE*5]\n",
    "                \n",
    "        ## shuffle\n",
    "        idx = random.sample(range(len(self.image_names)), len(self.image_names))\n",
    "        self.image_names = [self.image_names[i] for i in idx]\n",
    "        self.record_list = [self.record_list[i] for i in idx]\n",
    "        self.object_num_list = [self.object_num_list[i] for i in idx]\n",
    "\n",
    "    def _data_preprocess(self, image_name, raw_labels, object_num):\n",
    "        image_file = tf.io.read_file(IMAGE_DIR+image_name)\n",
    "        image = tf.io.decode_jpeg(image_file, channels=3)\n",
    "\n",
    "        h = tf.shape(image)[0]\n",
    "        w = tf.shape(image)[1]\n",
    "\n",
    "        width_rate = IMAGE_SIZE * 1.0 / tf.cast(w, tf.float32) \n",
    "        height_rate = IMAGE_SIZE * 1.0 / tf.cast(h, tf.float32) \n",
    "\n",
    "        image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "\n",
    "        raw_labels = tf.cast(tf.reshape(raw_labels, [-1, 5]), tf.float32)\n",
    "\n",
    "        xmin = raw_labels[:, 0]\n",
    "        ymin = raw_labels[:, 1]\n",
    "        xmax = raw_labels[:, 2]\n",
    "        ymax = raw_labels[:, 3]\n",
    "        class_num = raw_labels[:, 4]\n",
    "\n",
    "        xcenter = (xmin + xmax) * 1.0 / 2.0 * width_rate\n",
    "        ycenter = (ymin + ymax) * 1.0 / 2.0 * height_rate\n",
    "\n",
    "        box_w = (xmax - xmin) * width_rate\n",
    "        box_h = (ymax - ymin) * height_rate\n",
    "\n",
    "        image, xcenter, ycenter = random_flip(image, xcenter, ycenter)\n",
    "\n",
    "        labels = tf.stack([xcenter, ycenter, box_w, box_h, class_num], axis = 1)\n",
    "\n",
    "        image, labels = data_aug(image, labels)\n",
    "        image = tf.keras.applications.xception.preprocess_input(image)\n",
    "\n",
    "        return image, labels, tf.cast(object_num, tf.int32)\n",
    "\n",
    "    def generate(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.image_names, \n",
    "                                                      np.array(self.record_list), \n",
    "                                                      np.array(self.object_num_list)))\n",
    "        dataset = dataset.map(self._data_preprocess, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(1000)\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "        \n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_leaky_relu(inputs, filters, size, stride):\n",
    "    x = layers.Conv2D(filters, size, stride)(inputs)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = tf.keras.applications.xception.Xception(include_top=False,\n",
    "                                                            weights='imagenet',input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "pre_trained_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_mdl = keras.models.Sequential()\n",
    "\n",
    "inputs_mdl.add(layers.Conv2D(1024, (3,3), padding='same', activation='relu'))\n",
    "inputs_mdl.add(layers.MaxPool2D(pool_size=3,strides=2,padding='same'))\n",
    "inputs_mdl.add(layers.BatchNormalization())\n",
    "inputs_mdl.add(layers.Conv2D(1024, (3,3), padding='same', activation='relu'))\n",
    "inputs_mdl.add(layers.MaxPool2D(pool_size=3,strides=2,padding='same'))\n",
    "inputs_mdl.add(layers.BatchNormalization())\n",
    "\n",
    "inputs_mdl.add(layers.Flatten())\n",
    "inputs_mdl.add(layers.Dense(4096, kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.01)))\n",
    "inputs_mdl.add(layers.Dropout(0.5))\n",
    "inputs_mdl.add(layers.Dense(512, kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.01)))\n",
    "inputs_mdl.add(layers.Dropout(0.5))\n",
    "inputs_mdl.add(layers.Dense(CELL_SIZE*CELL_SIZE*(BOXES_PER_CELL*5+20),\n",
    "                            activation='relu', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.01)))\n",
    "\n",
    "YOLO = keras.Model(inputs=pre_trained_model.input, outputs=inputs_mdl(pre_trained_model.output), name=\"YOLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練模型的方法主要參考助教所提供的程式碼。\n",
    "<br/> 我們其他的修改與設定如下：\n",
    "1. 因為有對圖片進行翻轉，將圖片翻轉後會在計算loss的時候出現Error，因此有對部分計算loss的程式碼進行修改。\n",
    "2. 為了避免因為learning rate設的太大而發生loss明顯上升的情況，設定若當下的loss超過最低的loss兩倍時則提前停止，但在訓練的過程中沒有發生此情況。\n",
    "3. 根據預測結果，若使用imbalance的資料大約會在150個Epoch附近的時候結果較好，因此根據資料量推算，使用Balance data時，我們訓練的Epoch數為15個Epoch。\n",
    "4. 我們一開始有嘗試使用fine-tune，即在前面幾個epoch時，凍結住Pretrained的layers，等經過幾個epoch後，再一起訓練Pretrained的layers。在未使用balance data時，這個方法能夠有不錯的效果，但在使用balance data後，則是全部凍結住Pretrained的layers的效果較好，因此我們後來便都是以凍結住Pretrained的layers的方式進行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base boxes (for loss calculation)\n",
    "base_boxes = np.zeros([CELL_SIZE, CELL_SIZE, 4])\n",
    "\n",
    "#for each cell\n",
    "for y in range(CELL_SIZE):\n",
    "    for x in range(CELL_SIZE):\n",
    "        base_boxes[y, x, :] = [IMAGE_SIZE / CELL_SIZE * x, IMAGE_SIZE / CELL_SIZE * y, 0, 0]\n",
    "\n",
    "base_boxes = np.tile(np.resize(base_boxes, [CELL_SIZE, CELL_SIZE, 1, 4]), [1, 1, BOXES_PER_CELL, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(predicts, labels, objects_num):\n",
    "    \"\"\"\n",
    "    calculate loss\n",
    "    Args:\n",
    "        predict: 3-D tensor [cell_size, cell_size, 5 * boxes_per_cell]\n",
    "        labels : [max_objects, 5]  (x_center, y_center, w, h, class)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Add Loss to all the trainable variables\n",
    "    Args:\n",
    "        predicts: 4-D tensor [batch_size, cell_size, cell_size, 5 * boxes_per_cell]\n",
    "        ===> (num_classes, boxes_per_cell, 4 * boxes_per_cell)\n",
    "        labels  : 3-D tensor of [batch_size, max_objects, 5]\n",
    "        objects_num: 1-D tensor [batch_size]\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = predicts.shape[0]\n",
    "    loss = 0.\n",
    "\n",
    "    for i in tf.range(batch_size):\n",
    "        predict = predicts[i, :, :, :]\n",
    "        label = labels[i, :, :]\n",
    "        object_num = objects_num[i]\n",
    "\n",
    "        for j in tf.range(object_num):\n",
    "            results = losses_calculation(predict, label[j:j+1, :])\n",
    "            loss = loss + results\n",
    "\n",
    "    return loss/BATCH_SIZE\n",
    "\n",
    "def iou(boxes1, boxes2):\n",
    "    \"\"\"calculate ious\n",
    "    Args:\n",
    "      boxes1: 4-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n",
    "      boxes2: 1-D tensor [4] ===> (x_center, y_center, w, h)\n",
    "\n",
    "    Return:\n",
    "      iou: 3-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    \"\"\"\n",
    "\n",
    "    #boxes1 : [4(xmin, ymin, xmax, ymax), cell_size, cell_size, boxes_per_cell]\n",
    "    boxes1 = tf.stack([boxes1[:, :, :, 0] - boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] - boxes1[:, :, :, 3] / 2,\n",
    "                      boxes1[:, :, :, 0] + boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] + boxes1[:, :, :, 3] / 2])\n",
    "\n",
    "    #boxes1 : [cell_size, cell_size, boxes_per_cell, 4(xmin, ymin, xmax, ymax)]\n",
    "    boxes1 = tf.transpose(boxes1, [1, 2, 3, 0])\n",
    "\n",
    "    boxes2 =  tf.stack([boxes2[0] - boxes2[2] / 2, boxes2[1] - boxes2[3] / 2,\n",
    "                      boxes2[0] + boxes2[2] / 2, boxes2[1] + boxes2[3] / 2])\n",
    "\n",
    "    #calculate the left up point of boxes' overlap area\n",
    "    lu = tf.maximum(boxes1[:, :, :, 0:2], boxes2[0:2])\n",
    "    #calculate the right down point of boxes overlap area\n",
    "    rd = tf.minimum(boxes1[:, :, :, 2:], boxes2[2:])\n",
    "\n",
    "    #intersection\n",
    "    intersection = rd - lu \n",
    "\n",
    "    #the size of the intersection area\n",
    "    inter_square = intersection[:, :, :, 0] * intersection[:, :, :, 1]\n",
    "\n",
    "    mask = tf.cast(intersection[:, :, :, 0] > 0, tf.float32) * tf.cast(intersection[:, :, :, 1] > 0, tf.float32)\n",
    "\n",
    "    #if intersection is negative, then the boxes don't overlap\n",
    "    inter_square = mask * inter_square\n",
    "\n",
    "    #calculate the boxs1 square and boxs2 square\n",
    "    square1 = (boxes1[:, :, :, 2] - boxes1[:, :, :, 0]) * (boxes1[:, :, :, 3] - boxes1[:, :, :, 1])\n",
    "    square2 = (boxes2[2] - boxes2[0]) * (boxes2[3] - boxes2[1])\n",
    "\n",
    "    return inter_square/(square1 + square2 - inter_square + 1e-6)\n",
    "\n",
    "def losses_calculation(predict, label):\n",
    "    \"\"\"\n",
    "    calculate loss\n",
    "    Args:\n",
    "      predict: 3-D tensor [cell_size, cell_size, 5 * boxes_per_cell]\n",
    "      label : [1, 5]  (x_center, y_center, w, h, class)\n",
    "    \"\"\"\n",
    "    label = tf.reshape(label, [-1])\n",
    "\n",
    "    #calculate objects tensor [CELL_SIZE, CELL_SIZE]\n",
    "    min_x = (label[0] - label[2] / 2) / (IMAGE_SIZE / CELL_SIZE)\n",
    "    max_x = (label[0] + label[2] / 2) / (IMAGE_SIZE / CELL_SIZE)\n",
    "\n",
    "    min_y = (label[1] - label[3] / 2) / (IMAGE_SIZE / CELL_SIZE)\n",
    "    max_y = (label[1] + label[3] / 2) / (IMAGE_SIZE / CELL_SIZE)\n",
    "\n",
    "    # min_x = tf.floor(min_x)\n",
    "    # min_y = tf.floor(min_y)\n",
    "\n",
    "    min_x = tf.maximum(tf.math.floor(min_x), 0) #\n",
    "    min_y = tf.maximum(tf.math.floor(min_y), 0) #\n",
    "\n",
    "    max_x = tf.minimum(tf.math.ceil(max_x), CELL_SIZE)\n",
    "    max_y = tf.minimum(tf.math.ceil(max_y), CELL_SIZE)\n",
    "\n",
    "    temp = tf.cast(tf.stack([max_y - min_y, max_x - min_x]), dtype=tf.int32)\n",
    "    objects = tf.ones(temp, tf.float32)\n",
    "\n",
    "    temp = tf.cast(tf.stack([min_y, CELL_SIZE - max_y, min_x, CELL_SIZE - max_x]), tf.int32)\n",
    "    temp = tf.reshape(temp, (2, 2))\n",
    "    objects = tf.pad(objects, temp, \"CONSTANT\")\n",
    "\n",
    "    #calculate objects  tensor [CELL_SIZE, CELL_SIZE]\n",
    "    #calculate responsible tensor [CELL_SIZE, CELL_SIZE]\n",
    "    center_x = label[0] / (IMAGE_SIZE / CELL_SIZE)\n",
    "    center_x = tf.floor(center_x)\n",
    "\n",
    "    center_y = label[1] / (IMAGE_SIZE / CELL_SIZE)\n",
    "    center_y = tf.floor(center_y)\n",
    "\n",
    "    response = tf.ones([1, 1], tf.float32)\n",
    "\n",
    "    temp = tf.cast(tf.stack([center_y, CELL_SIZE - center_y - 1, \n",
    "                             center_x, CELL_SIZE - center_x - 1]), \n",
    "                   tf.int32)\n",
    "#     tmp = tf.stack([center_y, CELL_SIZE - center_y - 1,\n",
    "#                     center_x, CELL_SIZE - center_x - 1])\n",
    "    temp = tf.reshape(temp, (2, 2))\n",
    "    response = tf.pad(response, temp, \"CONSTANT\")\n",
    "    #objects = response\n",
    "\n",
    "    #calculate iou_predict_truth [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    predict_boxes = predict[:, :, NUM_CLASSES + BOXES_PER_CELL:]\n",
    "\n",
    "    predict_boxes = tf.reshape(predict_boxes, [CELL_SIZE, \n",
    "                                               CELL_SIZE, \n",
    "                                               BOXES_PER_CELL, 4])\n",
    "\n",
    "    predict_boxes = predict_boxes * [IMAGE_SIZE / CELL_SIZE, \n",
    "                                     IMAGE_SIZE / CELL_SIZE, \n",
    "                                     IMAGE_SIZE, IMAGE_SIZE]\n",
    "\n",
    "    #if there's no predict_box in that cell, then the base_boxes will be calcuated with label and got iou equals 0\n",
    "    predict_boxes = base_boxes + predict_boxes\n",
    "\n",
    "    iou_predict_truth = iou(predict_boxes, label[0:4])\n",
    "    #calculate C [cell_size, cell_size, boxes_per_cell]\n",
    "    C = iou_predict_truth * tf.reshape(response, [CELL_SIZE, CELL_SIZE, 1])\n",
    "\n",
    "    #calculate I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    I = iou_predict_truth * tf.reshape(response, (CELL_SIZE, CELL_SIZE, 1))\n",
    "\n",
    "    max_I = tf.reduce_max(I, 2, keepdims=True)\n",
    "\n",
    "    I = tf.cast((I >= max_I), tf.float32) * tf.reshape(response, (CELL_SIZE, CELL_SIZE, 1))\n",
    "\n",
    "    #calculate no_I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    no_I = tf.ones_like(I, dtype=tf.float32) - I\n",
    "\n",
    "    p_C = predict[:, :, NUM_CLASSES:NUM_CLASSES + BOXES_PER_CELL]\n",
    "\n",
    "    #calculate truth x, y, sqrt_w, sqrt_h 0-D\n",
    "    x = label[0]\n",
    "    y = label[1]\n",
    "\n",
    "    sqrt_w = tf.sqrt(tf.abs(label[2]))\n",
    "    sqrt_h = tf.sqrt(tf.abs(label[3]))\n",
    "\n",
    "    #calculate predict p_x, p_y, p_sqrt_w, p_sqrt_h 3-D [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    p_x = predict_boxes[:, :, :, 0]\n",
    "    p_y = predict_boxes[:, :, :, 1]\n",
    "\n",
    "    #p_sqrt_w = tf.sqrt(tf.abs(predict_boxes[:, :, :, 2])) * ((tf.cast(predict_boxes[:, :, :, 2] > 0, tf.float32) * 2) - 1)\n",
    "    #p_sqrt_h = tf.sqrt(tf.abs(predict_boxes[:, :, :, 3])) * ((tf.cast(predict_boxes[:, :, :, 3] > 0, tf.float32) * 2) - 1)\n",
    "    #p_sqrt_w = tf.sqrt(tf.maximum(0.0, predict_boxes[:, :, :, 2]))\n",
    "    #p_sqrt_h = tf.sqrt(tf.maximum(0.0, predict_boxes[:, :, :, 3]))\n",
    "    #p_sqrt_w = predict_boxes[:, :, :, 2]\n",
    "    #p_sqrt_h = predict_boxes[:, :, :, 3]\n",
    "    p_sqrt_w = tf.sqrt(tf.minimum(IMAGE_SIZE * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 2])))\n",
    "    p_sqrt_h = tf.sqrt(tf.minimum(IMAGE_SIZE * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 3])))\n",
    "\n",
    "    #calculate truth p 1-D tensor [NUM_CLASSES]\n",
    "    P = tf.one_hot(tf.cast(label[4], tf.int32), NUM_CLASSES, dtype=tf.float32)\n",
    "\n",
    "    #calculate predict p_P 3-D tensor [CELL_SIZE, CELL_SIZE, NUM_CLASSES]\n",
    "    p_P = predict[:, :, 0:NUM_CLASSES]\n",
    "\n",
    "    #class_loss\n",
    "    class_loss = tf.nn.l2_loss(tf.reshape(objects, (CELL_SIZE, CELL_SIZE, 1)) * (p_P - P)) * CLASS_SCALE\n",
    "    #class_loss = tf.nn.l2_loss(tf.reshape(response, (CELL_SIZE, CELL_SIZE, 1)) * (p_P - P)) * CLASS_SCALE\n",
    "\n",
    "    #object_loss\n",
    "    object_loss = tf.nn.l2_loss(I * (p_C - C)) * OBJECT_SCALE\n",
    "    #object_loss = tf.nn.l2_loss(I * (p_C - (C + 1.0)/2.0)) * OBJECT_SCALE\n",
    "\n",
    "    #noobject_loss\n",
    "    #noobject_loss = tf.nn.l2_loss(no_I * (p_C - C)) * NOOBJECT_SCALE\n",
    "    noobject_loss = tf.nn.l2_loss(no_I * (p_C)) * NOOBJECT_SCALE\n",
    "\n",
    "    #coord_loss\n",
    "    coord_loss = (tf.nn.l2_loss(I * (p_x - x)/(IMAGE_SIZE/CELL_SIZE)) +\n",
    "                 tf.nn.l2_loss(I * (p_y - y)/(IMAGE_SIZE/CELL_SIZE)) +\n",
    "                 tf.nn.l2_loss(I * (p_sqrt_w - sqrt_w))/IMAGE_SIZE +\n",
    "                 tf.nn.l2_loss(I * (p_sqrt_h - sqrt_h))/IMAGE_SIZE) * COORD_SCALE\n",
    "\n",
    "    return class_loss + object_loss + noobject_loss + coord_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetGenerator().generate()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "train_loss_metric = tf.keras.metrics.Mean(name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ckp = tf.train.latest_checkpoint(CKT_Dir)\n",
    "if last_ckp:\n",
    "    ckpt = tf.train.Checkpoint(net=YOLO)\n",
    "    ckpt.restore(last_ckp)\n",
    "    init_epoch = int(last_ckp.split(\"-\")[-1])+1\n",
    "    print(f'Resume training from epoch {init_epoch-1}') \n",
    "else:\n",
    "    init_epoch=1\n",
    "    print(\"Strat from 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1e10\n",
    "best_epoch = 0\n",
    "\n",
    "for f in os.listdir(CKT_Dir):\n",
    "    if \"BEST_\" in f:\n",
    "        index_dot = f.index(\".\")\n",
    "        index_ep = f.index(\"ep_\")\n",
    "        index_best = f.index(\"BEST_\")\n",
    "        \n",
    "        best_loss = int(f[index_ep + 3 : index_dot])/100\n",
    "        best_epoch = int(f[index_best + 5:index_ep])\n",
    "        break\n",
    "        \n",
    "print(\"Previous best epoch is {} wiht loss {}\".format(best_epoch, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(epoch=tf.Variable(init_epoch-1), net=YOLO)\n",
    "\n",
    "manager = tf.train.CheckpointManager(ckpt, CKT_Dir, max_to_keep=3,\n",
    "                                     checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, labels, objects_num):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = YOLO(image)\n",
    "        n1 = CELL_SIZE * CELL_SIZE * NUM_CLASSES\n",
    "        n2 = n1 + CELL_SIZE * CELL_SIZE * BOXES_PER_CELL\n",
    "        class_probs = tf.reshape(outputs[:, 0:n1], (-1, CELL_SIZE, CELL_SIZE, 20))\n",
    "        scales = tf.reshape(outputs[:, n1:n2], (-1, CELL_SIZE, CELL_SIZE, 2))\n",
    "        boxes = tf.reshape(outputs[:, n2:], (-1, CELL_SIZE, CELL_SIZE, 2*4))\n",
    "        predicts = tf.concat([class_probs, scales, boxes], 3)\n",
    "\n",
    "        loss = yolo_loss(predicts, labels, objects_num)\n",
    "        train_loss_metric(loss)\n",
    "\n",
    "    grads = tape.gradient(loss, YOLO.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, YOLO.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "epoch_list = []\n",
    "\n",
    "def remove_save_best_model(best_epoch, best_loss):\n",
    "    # remove old model\n",
    "    for f1 in os.listdir(CKT_Dir):\n",
    "        if \"BEST_\" in f1:\n",
    "            os.remove(CKT_Dir + \"/\" + f1)\n",
    "    \n",
    "    # save new model\n",
    "    for f in os.listdir(CKT_Dir):\n",
    "        if f.startswith(\"{}-{}\".format(checkpoint_name, best_epoch)):\n",
    "            new_file_name = f.replace(\"{}-{}\".format(checkpoint_name, best_epoch),\n",
    "                                      \"{}-BEST_{}ep_{}\".format(checkpoint_name,best_epoch, int(best_loss*100)))\n",
    "            copyfile(CKT_Dir + \"/\" + f, CKT_Dir + \"/\" + new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{}, start training.\".format(datetime.now()))\n",
    "\n",
    "for i in range(init_epoch,EPOCHS+1):\n",
    "    print(\"\\nEpoch: %d/%d\" % (i,EPOCHS))\n",
    "    train_loss_metric.reset_states()\n",
    "    ckpt.epoch.assign_add(1)\n",
    "\n",
    "    for idx, (image, labels, objects_num) in enumerate(dataset):\n",
    "        train_step(image, labels, objects_num)\n",
    "\n",
    "    tr_loss = train_loss_metric.result()\n",
    "    print(\"{}, Epoch {}: loss {:.5f}\".format(datetime.now(), i, tr_loss))\n",
    "    \n",
    "    epoch_list.append(i)\n",
    "    loss_list.append(tr_loss)\n",
    "    \n",
    "    if tr_loss > 2*best_loss:\n",
    "        print(\"Explosion at step %d\" % i)\n",
    "        break\n",
    "    \n",
    "    save_path = manager.save(checkpoint_number=tf.Variable(ckpt.epoch))\n",
    "    print(\"Saved checkpoint for epoch {}: {}\".format(int(ckpt.epoch), save_path))\n",
    "    \n",
    "    # Save best model\n",
    "    if tr_loss < best_loss:\n",
    "        best_loss = tr_loss\n",
    "        best_epoch = i\n",
    "        remove_save_best_model(best_epoch, best_loss)\n",
    "        print(\"New Best Epoch is {} with training loss {}\".format(best_epoch, best_loss))\n",
    "    \n",
    "Fin_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finish at {}\".format(Fin_time))\n",
    "print(\"The best epoch is {} wiht training loss {}\".format(best_epoch, best_loss))\n",
    "\n",
    "print(\"\\n-------------------------------------------------------\")\n",
    "for idx, loss_of_step in zip(epoch_list,loss_list):\n",
    "    print(\"The train loss of the %d-th epoch is %.3f\" % (idx, loss_of_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在助教提供的程式碼中，只會預測機率最大的Bounding Box與其類別，但在一張圖中可能會有許多不同的Object，因此必須要設定某個threshold，將confidence大於此threshold的bounding box預測出來。然而，可能會有多個bounding box框住同一個物體，所以必須要使用Non-max suppression的方式將多個高度重疊的bounding box選擇出一個。對於模型預測都是0的圖片則是只取一個。\n",
    "<br/> 我們便是實作了這個過程來預測一張圖片中的多個Object。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必須要注意的是，如果bounding box的面積為0，tensorflow的Non-max suppression會一直回傳這個面積為0的bounding box，因此，若有x_min等於x_max、y_min等於y_max的情況，我們會將x_max或y_max加0.01來避免此種情況。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_process_outputs(outputs):\n",
    "    \"\"\"\n",
    "    Process YOLO outputs into bou\n",
    "    \"\"\"\n",
    "\n",
    "    n1 = CELL_SIZE * CELL_SIZE * NUM_CLASSES\n",
    "    n2 = n1 + CELL_SIZE * CELL_SIZE * BOXES_PER_CELL\n",
    "    class_probs = np.reshape(outputs[:, 0:n1], (-1, CELL_SIZE, CELL_SIZE, 20))\n",
    "    scales = np.reshape(outputs[:, n1:n2], (-1, CELL_SIZE, CELL_SIZE, 2))\n",
    "    boxes = np.reshape(outputs[:, n2:], (-1, CELL_SIZE, CELL_SIZE, 2*4))\n",
    "    predicts = np.concatenate([class_probs, scales, boxes], 3)\n",
    "\n",
    "    p_classes = predicts[0, :, :, 0:20]\n",
    "    C = predicts[0, :, :, 20:22]\n",
    "    coordinate = predicts[0, :, :, 22:]\n",
    "\n",
    "    p_classes = np.reshape(p_classes, (CELL_SIZE, CELL_SIZE, 1, 20))\n",
    "    C = np.reshape(C, (CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 1))\n",
    "\n",
    "    P = C * p_classes\n",
    "    #P's shape [7, 7, 2, 20]\n",
    "    #print P[5,1, 0, :]\n",
    "\n",
    "    coordinate = np.reshape(coordinate, \n",
    "                            (CELL_SIZE, \n",
    "                              CELL_SIZE,\n",
    "                              BOXES_PER_CELL, \n",
    "                              4))\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    counter = np.sum(P>PROB_THRES)\n",
    "    if counter == 0:\n",
    "        counter = 1\n",
    "\n",
    "    while counter>0:\n",
    "        max_conf = np.max(P)\n",
    "        index = np.argmax(P)\n",
    "        index = np.unravel_index(index, P.shape)\n",
    "        assert P[index] == max_conf, \"兩個不合?\"\n",
    "\n",
    "        P[index[0],index[1],index[2],:] = 0.\n",
    "        class_num = index[3]\n",
    "\n",
    "        max_coordinate = coordinate[index[0], index[1], index[2], :]\n",
    "\n",
    "        xcenter = max_coordinate[0]\n",
    "        ycenter = max_coordinate[1]\n",
    "        w = max_coordinate[2]\n",
    "        h = max_coordinate[3]\n",
    "\n",
    "        xcenter = (index[1] + xcenter) * (IMAGE_SIZE/float(CELL_SIZE))\n",
    "        ycenter = (index[0] + ycenter) * (IMAGE_SIZE/float(CELL_SIZE))\n",
    "\n",
    "        w = w * IMAGE_SIZE\n",
    "        h = h * IMAGE_SIZE\n",
    "\n",
    "        xmin = xcenter - w/2.0\n",
    "        ymin = ycenter - h/2.0\n",
    "\n",
    "        xmax = xmin + w\n",
    "        ymax = ymin + h\n",
    "\n",
    "        counter = np.sum(P>PROB_THRES)\n",
    "        output.append([xmin, ymin, xmax, ymax, class_num, max_conf])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_files = open(PATH_TO_DATA+'/pascal_voc_testing_data.txt')\n",
    "test_img_dir = PATH_TO_DATA+'/VOCdevkit_test/VOCdevkit_test/VOC2007/JPEGImages/'\n",
    "test_images = []\n",
    "\n",
    "for line in test_img_files:\n",
    "    line = line.strip()\n",
    "    ss = line.split(' ')\n",
    "    test_images.append(ss[0])\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 152\n",
    "def load_img_data(image_name):\n",
    "    image_file = tf.io.read_file(test_img_dir+image_name)\n",
    "    image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "\n",
    "    h = tf.shape(image)[0]\n",
    "    w = tf.shape(image)[1]\n",
    "\n",
    "    image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
    "\n",
    "    return image_name, image, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception\n",
    "def load_img_data(image_name):\n",
    "    image_file = tf.io.read_file(test_img_dir+image_name)\n",
    "    image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "\n",
    "    h = tf.shape(image)[0]\n",
    "    w = tf.shape(image)[1]\n",
    "\n",
    "    image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image = tf.keras.applications.xception.preprocess_input(image)\n",
    "\n",
    "    return image_name, image, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(load_img_data, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_load_best:\n",
    "    for f in os.listdir(CKT_Dir):\n",
    "        if \"BEST_\" in f:\n",
    "            best_epoch_file = f[:f.index(\".\")]\n",
    "            break\n",
    "else:\n",
    "    best_epoch_file = checkpoint_name + \"-\" + str(EPOCHS)\n",
    "    \n",
    "print(\"Load checkpoint: \",best_epoch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(net=YOLO)\n",
    "ckpt.restore(CKT_Dir + \"/\" + best_epoch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def prediction_step(img):\n",
    "    return YOLO(img)\n",
    "\n",
    "@tf.function\n",
    "def tf_non_max(boxes,scores,max_value):\n",
    "    return tf.image.non_max_suppression(boxes,\n",
    "                                        scores,\n",
    "                                        max_value,\n",
    "                                        iou_threshold=IOU_THRES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapBox(box):\n",
    "    box[:,[0,1,2,3]] = box[:,[1,0,3,2]]\n",
    "    return box\n",
    "\n",
    "def execute_non_max(ori_box, ori_score, ori_class):\n",
    "    # ori_class => number\n",
    "    boxes = tf.Variable(ori_box, dtype=tf.float32)\n",
    "    scores = tf.Variable(ori_score, dtype=tf.float32)\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(boxes, scores,\n",
    "                                                    20, iou_threshold=IOU_THRES)\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy().tolist()\n",
    "    selected_class = [ori_class for _ in range(selected_boxes.shape[0])]\n",
    "    \n",
    "    return selected_boxes, selected_scores, selected_class\n",
    "\n",
    "def cal_non_max(original_box, class_number, conf):\n",
    "    original_box = swapBox(original_box)\n",
    "    \n",
    "    selected_boxes = np.array([[0., 0., 0., 0.]])\n",
    "    selected_scores = []\n",
    "    selected_class = []\n",
    "    \n",
    "    class_set = set(class_number)\n",
    "    for i in class_set:\n",
    "        curr_idx = [j for j in range(len(class_number)) if class_number[j] == i]\n",
    "        curr_boxes = original_box[curr_idx,:]\n",
    "        curr_scores = [conf[j] for j in curr_idx]\n",
    "        \n",
    "        curr_selected_boxes, curr_selected_scores, curr_selected_class = execute_non_max(curr_boxes, curr_scores, i)\n",
    "        \n",
    "        selected_boxes = np.concatenate((selected_boxes, curr_selected_boxes), axis=0)\n",
    "        selected_scores.extend(curr_selected_scores)\n",
    "        selected_class.extend(curr_selected_class)\n",
    "    \n",
    "    selected_boxes = swapBox(selected_boxes)\n",
    "    selected_boxes = selected_boxes[1:, :]\n",
    "    \n",
    "    tmp_scores = np.array(selected_scores)\n",
    "    order_list = tmp_scores.argsort().tolist()\n",
    "    order_list = order_list[::-1]\n",
    "    \n",
    "    output_boxes = selected_boxes[order_list,:]\n",
    "    output_scores = [selected_scores[j] for j in order_list]\n",
    "    output_class = [selected_class[j] for j in order_list]\n",
    "    \n",
    "    return output_boxes.tolist(), output_scores, output_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 水平翻轉預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們認為模型既然可能看得懂一邊的臉，但卻看不懂另外一邊，因此我們在訓練模型的時候要做Data augmentation，那我們也可以將這個想法應用在預測的階段。\n",
    "<br/> 因此，我們在進行預測的時候，我們先預測testing data的圖。接著，我們對圖進行水平翻轉，對水平翻轉後的圖再進行一次預測，並將新預測出的bounding box再翻轉回原始的圖上，並使用Non-max suppression將有進行水平翻轉預測出的bounding box與沒有進行翻轉預測出的bounding box合併。\n",
    "<br/> 如此一來，即便模型真的發生看不懂一邊的臉的情況，此方法也能夠讓模型預測到另一邊的臉，藉此來框出此Object。\n",
    "<br/>根據預測出的結果，我們也發現有許多原本預測都是0的圖，藉由這個方式而框出了Object。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必須要注意的是，因為這個方法整併了兩種預測結果，因此若有其中一個預測到錯誤的物件會導致最後的預測結果中也會有錯誤的物件，所以我們認為若將confidence的threshold適當的提高可以有更好的結果，因此我們將原本使用的threshold 0.001調整到0.01。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def flip_mirror(image):\n",
    "    return tf.image.flip_left_right(image)\n",
    "\n",
    "def clean_box(box,conf,cla):\n",
    "    if len(box)==0:\n",
    "        return box, conf,cla\n",
    "    \n",
    "    mask = []\n",
    "    for i in range(len(box)):\n",
    "        if int(box[i][0]) != int(box[i][2]) and int(box[i][1]) != int(box[i][3]) and conf[i] !=0:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    \n",
    "    if sum(mask) == 0:\n",
    "        output_box = [[0., 0., 0., 0.]]\n",
    "        output_conf = [0]\n",
    "        output_cla = [0]\n",
    "    else:\n",
    "        output_box = [box[j] for j in range(len(box)) if mask[j]]\n",
    "        output_conf = [conf[j] for j in range(len(conf)) if mask[j]]\n",
    "        output_cla = [cla[j] for j in range(len(cla)) if mask[j]]\n",
    "    \n",
    "    return output_box, output_conf, output_cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('./test_prediction_Resnet_paper.txt', 'w')\n",
    "# output_file = open('./test_prediction_Resnet.txt', 'w')\n",
    "# output_file = open('./test_prediction_Xception_paper.txt', 'w')\n",
    "# output_file = open('./test_prediction_Xception.txt', 'w')\n",
    "\n",
    "for img_name, test_img, img_h, img_w in test_dataset:\n",
    "    batch_num = img_name.shape[0]\n",
    "    for i in range(batch_num):\n",
    "        temp_original = multiple_process_outputs(prediction_step(test_img[i:i+1]))\n",
    "        temp_mirror = multiple_process_outputs(prediction_step(flip_mirror(test_img[i:i+1])))\n",
    "        \n",
    "        for idx in range(len(temp_mirror)):\n",
    "            tmp_x_max = IMAGE_SIZE-temp_mirror[idx][0]\n",
    "            tmp_x_min = IMAGE_SIZE-temp_mirror[idx][2]\n",
    "\n",
    "            temp_mirror[idx][0] = tmp_x_min\n",
    "            temp_mirror[idx][2] = tmp_x_max\n",
    "\n",
    "        temp = temp_original+temp_mirror\n",
    "        \n",
    "        now_w = img_w[i:i+1].numpy().tolist()[0]\n",
    "        now_h = img_h[i:i+1].numpy().tolist()[0]\n",
    "\n",
    "        box_np = np.zeros((len(temp),4))\n",
    "        class_num_list = []\n",
    "        conf_list = []\n",
    "\n",
    "        for j in range(len(temp)):\n",
    "            pred = temp[j]\n",
    "            xmin, ymin, xmax, ymax, class_num, conf = pred[0], pred[1], pred[2], pred[3], pred[4], pred[5]\n",
    "\n",
    "            xmin = max(xmin, 0)\n",
    "            ymin = max(ymin, 0)\n",
    "            xmax = min(xmax, IMAGE_SIZE)\n",
    "            ymax = min(ymax, IMAGE_SIZE)\n",
    "\n",
    "            # to avoid some weird problem\n",
    "            if xmin == xmax:\n",
    "                xmax += 0.1\n",
    "            if ymin == ymax:\n",
    "                ymax += 0.1\n",
    "\n",
    "            box_np[j,:] = [xmin*(now_w/IMAGE_SIZE), ymin*(now_h/IMAGE_SIZE), xmax*(now_w/IMAGE_SIZE), ymax*(now_h/IMAGE_SIZE)]\n",
    "            class_num_list.append(class_num)\n",
    "            conf_list.append(conf)\n",
    "\n",
    "        if len(conf_list)==1:\n",
    "            output_box, output_conf, output_class = box_np.tolist(), conf_list, class_num_list\n",
    "        else:\n",
    "            output_box, output_conf, output_class = cal_non_max(box_np, class_num_list, conf_list)\n",
    "\n",
    "        assert len(output_box)==len(output_conf)==len(output_class), \"長度不同\"\n",
    "        \n",
    "        output_box, output_conf, output_class = clean_box(output_box, output_conf, output_class)\n",
    "\n",
    "        # start output\n",
    "        output_file.write(img_name[i:i+1].numpy()[0].decode('ascii'))\n",
    "        for k in range(len(output_class)):\n",
    "            # for every box\n",
    "            now_box = output_box[k]\n",
    "            output_file.write(\" %d %d %d %d %d %f\" % (now_box[0], now_box[1], now_box[2],\n",
    "                                                      now_box[3], output_class[k], output_conf[k]))\n",
    "            \n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "output_file.close()\n",
    "print(\"Finish Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後，透過水平翻轉預測中使用Non-max suppression合併有無水平翻轉的bounding box的技巧。我們想到這個方法照理來說也可以用來合併不同模型所預測出的bounding box。\n",
    "<br/> 因此，我們便將不同的模型所預測出的Bounding box使用Non-max suppression進行合併，並將合併後的結果做為最終的預測結果。我們合併的模型如下：\n",
    "1. Paper的Augmentation + ResNet 152\n",
    "2. 我們的Augmentation + ResNet 152\n",
    "3. Paper的Augmentation + Xception\n",
    "4. 我們的Augmentation + Xception\n",
    "\n",
    "在合併的過程中，我們先將同為使用ResNet 152的第一個模型與第二個模型合併、將同為使用Xception的第三個模型與第四個模型合併，最後再將這兩個合併的結果合併以形成最終預測結果。\n",
    "<br/> 最終預測結果的Public Score為0.37759、Private Score為0.38902，都是我們所有嘗試的方法當中最好的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapBox(box):\n",
    "    box[:,[0,1,2,3]] = box[:,[1,0,3,2]]\n",
    "    return box\n",
    "\n",
    "def execute_non_max(ori_box, ori_score, ori_class):\n",
    "    # ori_class => number\n",
    "    boxes = tf.Variable(ori_box, dtype=tf.float32)\n",
    "    scores = tf.Variable(ori_score, dtype=tf.float32)\n",
    "    \n",
    "    selected_indices = tf.image.non_max_suppression(boxes, scores,\n",
    "                                                    20, iou_threshold=IOU_THRES)\n",
    "    \n",
    "    selected_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "    selected_scores = tf.gather(scores, selected_indices).numpy().tolist()\n",
    "    selected_class = [ori_class for _ in range(selected_boxes.shape[0])]\n",
    "    \n",
    "    return selected_boxes, selected_scores, selected_class\n",
    "\n",
    "def cal_non_max(original_box, class_number, conf):\n",
    "    original_box = swapBox(original_box)\n",
    "    \n",
    "    selected_boxes = np.array([[0., 0., 0., 0.]])\n",
    "    selected_scores = []\n",
    "    selected_class = []\n",
    "    \n",
    "    class_set = set(class_number)\n",
    "    for i in class_set:\n",
    "        curr_idx = [j for j in range(len(class_number)) if class_number[j] == i]\n",
    "        curr_boxes = original_box[curr_idx,:]\n",
    "        curr_scores = [conf[j] for j in curr_idx]\n",
    "        \n",
    "        curr_selected_boxes, curr_selected_scores, curr_selected_class = execute_non_max(curr_boxes, curr_scores, i)\n",
    "        \n",
    "        selected_boxes = np.concatenate((selected_boxes, curr_selected_boxes), axis=0)\n",
    "        selected_scores.extend(curr_selected_scores)\n",
    "        selected_class.extend(curr_selected_class)\n",
    "    \n",
    "    selected_boxes = swapBox(selected_boxes)\n",
    "    selected_boxes = selected_boxes[1:, :]\n",
    "    \n",
    "    tmp_scores = np.array(selected_scores)\n",
    "    order_list = tmp_scores.argsort().tolist()\n",
    "    order_list = order_list[::-1]\n",
    "    \n",
    "    output_boxes = selected_boxes[order_list,:]\n",
    "    output_scores = [selected_scores[j] for j in order_list]\n",
    "    output_class = [selected_class[j] for j in order_list]\n",
    "    \n",
    "    return output_boxes.tolist(), output_scores, output_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble_obj_detect(object):\n",
    "    def __init__(self, file_list, path_to_file):\n",
    "        self.path_to_file = path_to_file\n",
    "        self.mapping_dict = {}\n",
    "\n",
    "        self.image_order = []\n",
    "\n",
    "        for f in file_list:\n",
    "            tmp_dict, tmp_image_order = self.read_file(f)\n",
    "            self.mapping_dict[f] = tmp_dict\n",
    "\n",
    "            if len(self.image_order) == 0:\n",
    "                self.image_order = tmp_image_order\n",
    "\n",
    "    def read_file(self, file_name):\n",
    "        output_dict = {}\n",
    "        output_image_order = []\n",
    "\n",
    "        with open(self.path_to_file + file_name) as f:\n",
    "            for line in f:\n",
    "                ss = line.strip().split()\n",
    "                image_name = ss[0]\n",
    "                line_data = ss[1:]\n",
    "\n",
    "                output_dict[image_name] = []\n",
    "                output_image_order.append(image_name)\n",
    "\n",
    "                for i in range(len(line_data) // 6):\n",
    "                    curr_data = [\n",
    "                        float(j) for j in line_data[6 * i:6 * (i + 1)]\n",
    "                    ]\n",
    "                    output_dict[image_name].append(curr_data)\n",
    "\n",
    "        return output_dict, output_image_order\n",
    "\n",
    "    def ensemble(self, weights=None):\n",
    "        self.reshape_dict = {}\n",
    "\n",
    "        for file_name in self.mapping_dict.keys():\n",
    "            tmp_dict = self.mapping_dict[file_name]\n",
    "\n",
    "            for image_name in tmp_dict.keys():\n",
    "                if image_name not in self.reshape_dict.keys():\n",
    "                    self.reshape_dict[image_name] = tmp_dict[image_name].copy()\n",
    "                else:\n",
    "                    self.reshape_dict[image_name].extend(tmp_dict[image_name])\n",
    "\n",
    "        self.perform_ensemble()\n",
    "        self.clean()\n",
    "\n",
    "    def perform_ensemble(self):\n",
    "        self.ensemble_dict = {}\n",
    "\n",
    "        for image_name in self.reshape_dict.keys():\n",
    "            tmp_array = np.array(self.reshape_dict[image_name])\n",
    "            boxes = tmp_array[:, :4]\n",
    "            class_list = tmp_array[:, 4].tolist()\n",
    "            scores_list = tmp_array[:, 5].tolist()\n",
    "\n",
    "            x_same_bool = boxes[:, 0] == boxes[:, 2]\n",
    "            boxes[x_same_bool, 2] += 0.5\n",
    "\n",
    "            y_same_bool = boxes[:, 1] == boxes[:, 3]\n",
    "            boxes[y_same_bool, 3] += 0.5\n",
    "\n",
    "            output_box, output_score, output_class = cal_non_max(\n",
    "                boxes, class_list, scores_list)\n",
    "\n",
    "            np_box = np.array(output_box)\n",
    "\n",
    "            np_score = np.array(output_score)\n",
    "            np_score = np_score.reshape((np_box.shape[0], 1))\n",
    "\n",
    "            np_class = np.array(output_class)\n",
    "            np_class = np_class.reshape((np_box.shape[0], 1))\n",
    "\n",
    "            output = np.concatenate((np_box, np_class, np_score), axis=1)\n",
    "\n",
    "            # sort array from max to min\n",
    "            output = output[output[:, 5].argsort()[::-1], :]\n",
    "            output = self.remove_low_confidence(output)\n",
    "\n",
    "            self.ensemble_dict[image_name] = output.tolist()\n",
    "\n",
    "    def clean(self):\n",
    "        for image_name in self.ensemble_dict.keys():\n",
    "            if len(self.ensemble_dict[image_name]) > 1:\n",
    "                new_list = []\n",
    "                for box in self.ensemble_dict[image_name]:\n",
    "                    if int(box[0]) != int(box[2]) and int(box[1]) != int(\n",
    "                            box[3]):\n",
    "                        new_list.append(box)\n",
    "\n",
    "                if len(new_list) == 0:\n",
    "                    new_list.append([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "                self.ensemble_dict[image_name] = new_list\n",
    "\n",
    "            assert len(self.ensemble_dict[image_name]) >= 1, \"沒有箱子?\"\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_low_confidence(whole_array):\n",
    "        if whole_array.shape[0] == 1:\n",
    "            return whole_array\n",
    "\n",
    "        high_conf = whole_array[:, 5] > PROB_THRES\n",
    "\n",
    "        if np.sum(high_conf) == 0:\n",
    "            # all low => return highest\n",
    "            return whole_array[[0], :]\n",
    "        else:\n",
    "            return whole_array[high_conf, :]\n",
    "\n",
    "    def write_ensemble(self, file_name):\n",
    "        with open(file_name, \"w\") as f:\n",
    "            for image_name in self.image_order:\n",
    "                f.write(image_name)\n",
    "\n",
    "                for to_write in self.ensemble_dict[image_name]:\n",
    "                    f.write(\" %d %d %d %d %d %.6f\" %\n",
    "                            (to_write[0], to_write[1], to_write[2],\n",
    "                             to_write[3], to_write[4], to_write[5]))\n",
    "\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_THRES=0.3\n",
    "PROB_THRES=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enser1 = ensemble_obj_detect(file_list=[\"test_prediction_Resnet_paper.txt\",\n",
    "                                        \"test_prediction_Resnet.txt\"],\n",
    "                             path_to_file=\"../data/output/txt/\")\n",
    "enser1.ensemble()\n",
    "enser1.write_ensemble(\"../data/output/txt/ensemble_Resnet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enser2 = ensemble_obj_detect(file_list=[\"test_prediction_Xception_paper.txt\",\n",
    "                                        \"test_prediction_Xception.txt\"],\n",
    "                             path_to_file=\"../data/output/txt/\")\n",
    "enser2.ensemble()\n",
    "enser2.write_ensemble(\"../data/output/txt/ensemble_Xception.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enser3 = ensemble_obj_detect(file_list=[\"ensemble_Resnet.txt\",\n",
    "                                        \"ensemble_Xception.txt\"],\n",
    "                             path_to_file=\"../data/output/txt/\")\n",
    "enser3.ensemble()\n",
    "enser3.write_ensemble(\"../data/output/txt/ensemble_final.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\n",
    "1. 這次的模型訓練動輒12小時起跳，因此必須要審慎思考必須要做的方法以及小心的coding，不然可能會發生train了很久最後結果卻有問題的情況。\n",
    "2. 不論是augmentation的方法或是模型的設計，paper所提供的方法都能夠有不錯的效果，因此，可以先參考paper的設計再從這點開始變化，會比從0開始嘗試來得快速、有效。\n",
    "3. 使用水平翻轉預測與ensemble可以得到不錯的成效，後來我有在Kaggle的有獎競賽中看到有人也會使用類似的方法，但每個人的設定與使用方式略有不同，或許可以多參考其他人的使用方式。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
