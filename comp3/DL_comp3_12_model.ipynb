{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbEEXjKA1u_4"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "# Install TensorFlow\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "    \n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hUQp8Cmv1u_-",
    "outputId": "59500c32-e831-4cca-886f-4ad165cc0593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDcBvJqM1vAD"
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../data/\"\n",
    "CKPT_DIR = \"../ckpts/final/\"\n",
    "IFERENCE_PATH = \"../inference/final/\"\n",
    "SAMPLE_DIR = \"../samples/final/\"\n",
    "\n",
    "checkpoint_name = \"final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_down_flip_p = 0.15\n",
    "left_right_flip_p = 0.5\n",
    "\n",
    "noise_p = 0.3\n",
    "rot_p = 0.4\n",
    "\n",
    "crop_p = 0.3\n",
    "brightness_p = 0.15\n",
    "saturation_p = 0.15\n",
    "contrast_p = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "eWafPGTU1vAF",
    "outputId": "ad4757e1-9dbf-4721-b92d-e3157e5fd0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = PATH_TO_DATA + 'dictionary/dictionary/'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "# print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "# print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "# print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8_DIeYcU1vAI",
    "outputId": "8ee31ac0-2cce-49bb-9e2a-3e21285a6315"
   },
   "outputs": [],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    \n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    \n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "def idList2sent(idList, is_int=True):\n",
    "    if is_int:\n",
    "        return \" \".join([id2word_dict[str(i)] for i in idList if id2word_dict[str(i)] != \"<PAD>\" and id2word_dict[str(i)] != \"<RARE>\"])\n",
    "    else:\n",
    "        return \" \".join([id2word_dict[i] for i in idList if id2word_dict[i] != \"<PAD>\" and id2word_dict[i] != \"<RARE>\"])\n",
    "    \n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "# print(text)\n",
    "# print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rAg5uK8S1vAL",
    "outputId": "79835413-8155-4c54-b190-1bc5bc4b1d9b"
   },
   "outputs": [],
   "source": [
    "data_path = PATH_TO_DATA + 'dataset/dataset/'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "# print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lO5U2f51vAO"
   },
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def get_all(sentence, tokenizer, max_seq_length):\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "    \n",
    "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(stokens, max_seq_length)\n",
    "    input_segments = get_segments(stokens, max_seq_length)\n",
    "    \n",
    "    return input_ids, input_masks, input_segments\n",
    "\n",
    "def to_bert(caption, max_seq_length=30):\n",
    "    caption_list = caption.numpy().tolist()\n",
    "    \n",
    "    result_ids = []\n",
    "    result_masks = []\n",
    "    result_segments = []\n",
    "    \n",
    "    for sen in caption_list:\n",
    "        tmp = idList2sent(sen)\n",
    "        \n",
    "        idx, mask, segment = get_all(tmp, tokenizer=TOKENIZER, max_seq_length=max_seq_length)\n",
    "        \n",
    "        result_ids.append(idx)\n",
    "        result_masks.append(mask)\n",
    "        result_segments.append(segment)\n",
    "        \n",
    "    return np.asarray(result_ids), np.asarray(result_masks), np.asarray(result_segments)\n",
    "\n",
    "def to_use(caption):\n",
    "    caption_list = caption.numpy()\n",
    "    \n",
    "    return np.asarray(idList2sent(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPx1LrEz1vAQ"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gaussian_noise(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "    \n",
    "    if prob < noise_p:\n",
    "        noise = tf.random.normal(image.shape, stddev=0.1)\n",
    "        image = tf.math.add(image, noise)\n",
    "        \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def random_flip(image):\n",
    "    up_down_outcome = tf.random.uniform([1], 0, 1)\n",
    "    right_left_outcome = tf.random.uniform([1], 0, 1)\n",
    "\n",
    "    if up_down_outcome<up_down_flip_p:\n",
    "        image = tf.image.flip_up_down(image)\n",
    "\n",
    "    if right_left_outcome<left_right_flip_p:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def random_rot90(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "    \n",
    "    if prob < (rot_p/2):\n",
    "        image = tf.image.rot90(image)\n",
    "    elif (prob >= (rot_p/2)) and (prob < rot_p):\n",
    "        image = tf.image.rot90(image, k=2)   \n",
    "        \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def r_crop(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "    \n",
    "    if prob < crop_p:\n",
    "        image = tf.image.resize(image, size=[IMAGE_HEIGHT + 5, IMAGE_WIDTH + 5])\n",
    "        image = tf.image.random_crop(image, size=[IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
    "    else:\n",
    "        image = tf.image.resize(image, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "        \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def brightness(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "    \n",
    "    if prob < brightness_p:\n",
    "        image = tf.image.random_brightness(image, 0.5)\n",
    "        \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def saturation(image):\n",
    "    prob = tf.random.uniform([1], 0, 1)\n",
    "    \n",
    "    if prob < saturation_p:\n",
    "        image = tf.image.random_saturation(image, 0.85, 1.5)\n",
    "        \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def contrast(image):\n",
    "    prob = tf.random.uniform([1], 0, 1)\n",
    "    \n",
    "    if prob < contrast_p:\n",
    "        image = tf.image.random_contrast(image, 0.9, 2)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Rrvkv9B1vAS"
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize_with_crop_or_pad(img, 500, 500)\n",
    "    img = r_crop(img)\n",
    "    # img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    img = contrast(img)\n",
    "    # img = brightness(img)\n",
    "    # img = saturation(img)\n",
    "    \n",
    "    # img = 2 * img - 1\n",
    "    img = random_flip(img)\n",
    "    img = random_rot90(img)\n",
    "    # img = gaussian_noise(img)\n",
    "    \n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    # caption = tf.py_function(func=to_use, inp=[caption], Tout=[tf.string])\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "def training_data_generator_only_image(caption, image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize_with_crop_or_pad(img, 500, 500)\n",
    "    img = r_crop(img)\n",
    "    # img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    img = contrast(img)\n",
    "    # img = brightness(img)\n",
    "    # img = saturation(img)\n",
    "    \n",
    "    # img = 2 * img - 1\n",
    "    img = random_flip(img)\n",
    "    img = random_rot90(img)\n",
    "    # img = gaussian_noise(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def training_data_generator_only_caption(caption, image_path):\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "class dataset_generator():\n",
    "    \n",
    "    def __init__(self, filenames, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        df = pd.read_pickle(filenames)\n",
    "        captions = df['Captions'].values\n",
    "        self.caption = []\n",
    "        for i in range(len(captions)):\n",
    "            self.caption.append(random.choice(captions[i]))\n",
    "        self.caption = np.asarray(self.caption)\n",
    "        self.caption = self.caption.astype(np.int)\n",
    "        self.image_path = df['ImagePath'].values\n",
    "        assert self.caption.shape[0] == self.image_path.shape[0]\n",
    "        \n",
    "    def generate(self, generate_func):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.caption, PATH_TO_DATA + \"102flowers/\"+ self.image_path))\n",
    "        dataset = dataset.map(generate_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(len(self.caption)).batch(self.batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-c-hByeT1vAV"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "ds_generator = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE)\n",
    "\n",
    "dataset = ds_generator.generate(training_data_generator)\n",
    "dataset_wrong_image = ds_generator.generate(training_data_generator_only_image)\n",
    "dataset_wrong_caption = ds_generator.generate(training_data_generator_only_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9U3SCxU1vAX"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        self.hidden_size = int(self.hparas['T_DIM'] / 2)\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hidden_size,\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "        self.bid = layers.Bidirectional(self.gru)\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output, state1, state2 = self.bid(text, initial_state = hidden)\n",
    "        return output[:, -1, :], state1\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.hparas['BATCH_SIZE'], self.hidden_size)) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_encoder(tf.keras.Model):\n",
    "    def __init__(self, inputs, is_training=True, reuse=False):\n",
    "        super(cnn_encoder, self).__init__()\n",
    "        # self.inputs = inputs\n",
    "        # self.is_training = is_training\n",
    "        # self.reuse = reuse\n",
    "        self.hparas = hparas\n",
    "        self.df_dim = 64\n",
    "        self.t_dim = self.hparas['T_DIM']\n",
    "        \n",
    "        self.net_h0_conv2d = tf.keras.layers.Conv2D(self.df_dim, 4, 2, padding='SAME',use_bias=True, kernel_initializer= tf.random_normal_initializer(stddev=0.02))\n",
    "        self.net_h1_conv2d = tf.keras.layers.Conv2D(self.df_dim*2, 4, 2, padding='SAME',use_bias=True, kernel_initializer= tf.random_normal_initializer(stddev=0.02))\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.net_h2_conv2d = tf.keras.layers.Conv2D(self.df_dim*4, 4, 2, padding='SAME',use_bias=True, kernel_initializer= tf.random_normal_initializer(stddev=0.02))\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.net_h3_conv2d = tf.keras.layers.Conv2D(self.df_dim*8, 4, 2, padding='SAME',use_bias=True, kernel_initializer= tf.random_normal_initializer(stddev=0.02))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.net_h4_flat = tf.keras.layers.Flatten()\n",
    "        self.net_h4_fc = tf.keras.layers.Dense(self.t_dim, use_bias=False, kernel_initializer= tf.random_normal_initializer(stddev=0.02))\n",
    "\n",
    "    def call(self, inputs, is_training = True):\n",
    "        net_h0 = self.net_h0_conv2d(inputs)\n",
    "        net_h0 = tf.nn.leaky_relu(net_h0)\n",
    "        \n",
    "        net_h1 = self.net_h1_conv2d(net_h0)\n",
    "        net_h1 = self.bn1(net_h1, training=is_training)\n",
    "        net_h1 = tf.nn.leaky_relu(net_h1)\n",
    "\n",
    "        net_h2 = self.net_h2_conv2d(net_h1)\n",
    "        net_h2 = self.bn2(net_h2, training=is_training)\n",
    "        net_h2 = tf.nn.leaky_relu(net_h2)\n",
    "\n",
    "        net_h3 = self.net_h3_conv2d(net_h2)\n",
    "        net_h3 = self.bn3(net_h3, training=is_training)\n",
    "        net_h3 = tf.nn.leaky_relu(net_h3)\n",
    "\n",
    "        net_h4 = self.net_h4_flat(net_h3)\n",
    "        net_h4 = self.net_h4_fc(net_h4)\n",
    "\n",
    "        outputs = net_h4\n",
    "\n",
    "        return outputs       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCJjHvfH1vAY"
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.t_dim = self.hparas['T_DIM'] * 2\n",
    "        self.gf_dim = 64\n",
    "        self.image_size = self.hparas['IMAGE_SIZE'][0]\n",
    "        self.c_dim = self.hparas['IMAGE_SIZE'][2]\n",
    "        # self.latent_shape = self.hparas[\"PRE_TRAIND_TEXT_EMBBEDING\"] + self.hparas['DENSE_DIM']\n",
    "        \n",
    "        self.s2, self.s4 = int(self.image_size/2), int(self.image_size/4)\n",
    "        self.s8, self.s16 = int(self.image_size/8), int(self.image_size/16)\n",
    "        \n",
    "        ######\n",
    "        self.net_txt_fc1 = tf.keras.layers.Dense(self.t_dim)\n",
    "        \n",
    "        self.h0_fc1 = tf.keras.layers.Dense(self.s16 * self.s16 * 8 * self.gf_dim)\n",
    "        self.h0_bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.reshape = tf.keras.layers.Reshape(target_shape = (self.s16, self.s16, 8 * self.gf_dim))\n",
    "        \n",
    "        self.h1_tConv1 = tf.keras.layers.Conv2DTranspose(self.gf_dim * 4, 5, 2, padding='SAME')\n",
    "        self.h1_bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.h2_tConv1 = tf.keras.layers.Conv2DTranspose(self.gf_dim * 2, 5, 2, padding='SAME')\n",
    "        self.h2_bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.h3_tConv1 = tf.keras.layers.Conv2DTranspose(self.gf_dim * 1, 5, 2, padding='SAME')\n",
    "        self.h3_bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.hout = tf.keras.layers.Conv2DTranspose(self.c_dim, 5, 2, padding='SAME')\n",
    "        \n",
    "    def call(self, text, noise_z, is_training=True):\n",
    "        net_txt = self.net_txt_fc1(text)\n",
    "        net_txt = tf.nn.leaky_relu(net_txt)\n",
    "        \n",
    "        mean_d = net_txt[:,:(tf.cast(self.t_dim/2,tf.int32))]\n",
    "        stddev = tf.math.exp(net_txt[:,(tf.cast(self.t_dim/2,tf.int32)):])\n",
    "        \n",
    "        if is_training:\n",
    "            epsilon = tf.random.normal(mean_d.shape)\n",
    "            net_txt = mean_d + stddev * epsilon\n",
    "        else:\n",
    "            net_txt = mean_d\n",
    "        \n",
    "        x = tf.concat([noise_z, net_txt], axis=1)\n",
    "        \n",
    "        # h0\n",
    "        x = self.h0_fc1(x)\n",
    "        \n",
    "        x = self.reshape(x)\n",
    "        x = self.h0_bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # h1\n",
    "        x = self.h1_tConv1(x)\n",
    "        x = self.h1_bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # h2\n",
    "        x = self.h2_tConv1(x)\n",
    "        x = self.h2_bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "    \n",
    "        # h3\n",
    "        x = self.h3_tConv1(x)\n",
    "        x = self.h3_bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # hout\n",
    "        logits = self.hout(x) # logits\n",
    "        output = tf.nn.tanh(logits)*0.5 + 0.5\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7eCp9fX1vAa"
   },
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.t_dim = self.hparas['T_DIM']\n",
    "        self.df_dim = 64\n",
    "        self.image_size = self.hparas['IMAGE_SIZE'][0]\n",
    "        \n",
    "        self.s2, self.s4 = int(self.image_size/2), int(self.image_size/4)\n",
    "        self.s8, self.s16 = int(self.image_size/8), int(self.image_size/16)\n",
    "        \n",
    "        self.h0_Conv1 = tf.keras.layers.Conv2D(self.df_dim, 5, 2, padding='SAME')\n",
    "\n",
    "        self.h1_Conv1 = tf.keras.layers.Conv2D(self.df_dim*2, 5, 2, padding='SAME')\n",
    "        self.h1_bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.h2_Conv1 = tf.keras.layers.Conv2D(self.df_dim*4, 5, 2, padding='SAME')\n",
    "        self.h2_bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.h3_Conv1 = tf.keras.layers.Conv2D(self.df_dim*8, 5, 2, padding='SAME')\n",
    "        self.h3_bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.net_txt_fc1 = tf.keras.layers.Dense(self.t_dim)\n",
    "\n",
    "        self.h4_Conv1 = tf.keras.layers.Conv2D(self.df_dim*8, 1, 1, padding='SAME')\n",
    "        self.h4_bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.hout = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, img, text):\n",
    "        x = self.h0_Conv1(img)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        x = self.h1_Conv1(x)\n",
    "        x = self.h1_bn1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        x = self.h2_Conv1(x)\n",
    "        x = self.h2_bn1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        x = self.h3_Conv1(x)\n",
    "        x = self.h3_bn1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        ####\n",
    "        net_txt = self.net_txt_fc1(text)\n",
    "        net_txt = tf.nn.leaky_relu(net_txt)\n",
    "        net_txt = tf.expand_dims(net_txt, axis=1)\n",
    "        net_txt = tf.expand_dims(net_txt, axis=2)\n",
    "        net_txt = tf.tile(net_txt, [1, 4, 4, 1])\n",
    "        \n",
    "        x = tf.concat([x, net_txt], axis=3)\n",
    "        \n",
    "        x = self.h4_Conv1(x)\n",
    "        x = self.h4_bn1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        logits = self.hout(x) # logits\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3BoW_Wx1vAc"
   },
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 128,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    \"T_DIM\": 64,\n",
    "    'RNN_HIDDEN_SIZE': 64,                    # number of RNN neurons (aligned with T_DIM)\n",
    "    'Z_DIM': 100,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 850,\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'CHECKPOINTS_DIR': CKPT_DIR,  # checkpoint path\n",
    "    'PRINT_FREQ': 1                        # printing frequency of loss                        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrUAmQ1J1vAg"
   },
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas)\n",
    "cnn_encoder = cnn_encoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yofHO45o1vAj"
   },
   "source": [
    "# Loss Function and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwKtqi-V1vAk"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def grad_pentalty(disc, x, real_embed):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = disc(x, real_embed)\n",
    "    \n",
    "    grad = tape.gradient(y, x)\n",
    "    norm_grad = tf.sqrt(tf.reduce_sum(grad ** 2, axis=[1, 2, 3]))\n",
    "    output = tf.reduce_mean((norm_grad - 1.0) ** 2)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3og_j-O1vAm"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits, mismatch_text_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(y_true=tf.ones_like(real_logits), y_pred=real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(y_true=tf.zeros_like(fake_logits), y_pred=fake_logits)\n",
    "    mismatch_text_loss = cross_entropy(y_true=tf.zeros_like(mismatch_text_logits), y_pred=mismatch_text_logits) # wrong text\n",
    "    # mismatch_image_loss = cross_entropy(y_true=tf.zeros_like(mismatch_image_logits), y_pred=mismatch_image_logits) # wrong image\n",
    "    total_loss = real_loss + ((fake_loss + mismatch_text_loss)/2)\n",
    "    # total_loss = real_loss + ((fake_loss + mismatch_text_loss + mismatch_image_loss)/3)\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    # return -tf.reduce_mean(fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "def cosine_similarity(v1, v2):\n",
    "    cost = tf.reduce_sum(tf.multiply(v1, v2), 1) / (tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)))\n",
    "    return cost\n",
    "\n",
    "def rnn_loss(real_cnn, text_embed, wrong_cnn, wrong_text_embed):\n",
    "    rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(real_cnn, text_embed) + cosine_similarity(real_cnn, wrong_text_embed))) + \\\n",
    "                tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(real_cnn, text_embed) + cosine_similarity(wrong_cnn, text_embed)))   \n",
    "    return rnn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_q1vHQvl1xej"
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CGTEkyO1vAo"
   },
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "rnn_optimizer = tf.keras.optimizers.Adam(hparas['LR'], clipnorm=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S0kGcGgu0zqL",
    "outputId": "6d20522f-fde6-4972-a458-60bdf64bdf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume training from epoch 900\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = hparas[\"CHECKPOINTS_DIR\"] \n",
    "\n",
    "last_ckp = tf.train.latest_checkpoint(checkpoint_path)\n",
    "start_epoch = 0\n",
    "\n",
    "if last_ckp:\n",
    "    ckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                               discriminator_optimizer=discriminator_optimizer,\n",
    "                               rnn_optimizer=rnn_optimizer,\n",
    "                               text_encoder=text_encoder,\n",
    "                               cnn_encoder=cnn_encoder,\n",
    "                               generator=generator,\n",
    "                               discriminator=discriminator)\n",
    "    ckpt.restore(last_ckp)\n",
    "    start_epoch = int(last_ckp.split(\"-\")[-1]) * 50\n",
    "\n",
    "print(f'Resume training from epoch {start_epoch}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYaFooPF1vAq"
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                           discriminator_optimizer=discriminator_optimizer,\n",
    "                           rnn_optimizer=rnn_optimizer,\n",
    "                           text_encoder=text_encoder,\n",
    "                           cnn_encoder=cnn_encoder,\n",
    "                           generator=generator,\n",
    "                           discriminator=discriminator)\n",
    "\n",
    "manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10,\n",
    "                                     checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dl2dE1iN1vAs"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, real_caption,real_hidden, wrong_image, wrong_caption,wrong_hidden):\n",
    "    # random noise for generator\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    # real_text_embed = text_encoder(real_caption)\n",
    "    # wrong_text_embed = text_encoder(wrong_caption)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        real_text_embed, hidden = text_encoder(real_caption, real_hidden)\n",
    "        wrong_text_embed, wrong_hidden = text_encoder(wrong_caption, wrong_hidden)\n",
    "        _, fake_image = generator(real_text_embed, noise)\n",
    "        \n",
    "        real_logits, real_output = discriminator(real_image, real_text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, real_text_embed)\n",
    "        mismatch_text_logits, mismatch_text_output = discriminator(real_image, wrong_text_embed)\n",
    "\n",
    "        g_loss = generator_loss(fake_logits)\n",
    "        d_loss = discriminator_loss(real_logits, fake_logits, mismatch_text_logits)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_rnn(real_image, real_caption, hidden, wrong_image, wrong_caption, wrong_hidden):\n",
    "    with tf.GradientTape() as rnn_tape:\n",
    "        x = cnn_encoder(real_image)\n",
    "        v, hidden = text_encoder(real_caption, hidden)\n",
    "        x_w = cnn_encoder(wrong_image)\n",
    "        v_w, mis_hidden = text_encoder(wrong_caption, wrong_hidden)\n",
    "\n",
    "        r_loss = rnn_loss(x, v, x_w, v_w)\n",
    "        \n",
    "    grad_r = rnn_tape.gradient(r_loss, text_encoder.trainable_variables + cnn_encoder.trainable_variables)\n",
    "    rnn_optimizer.apply_gradients(zip(grad_r, text_encoder.trainable_variables + cnn_encoder.trainable_variables))\n",
    "\n",
    "    return r_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eec5m7O1vAw"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise, is_training=False)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rt-Jo51-1vA0"
   },
   "source": [
    "# Visualiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqN7WrAG1vA5"
   },
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size))\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jz8CF-_81vBD"
   },
   "outputs": [],
   "source": [
    "def test_to_use(caption):\n",
    "    output = tf.py_function(func=to_use, inp=[caption], Tout=[tf.string])\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8rXJMlR1vBH"
   },
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    # dataset = dataset.map(test_to_use, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58mkSier1vBK"
   },
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0G-taON1vBM"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3IQYC7Tdzx-"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "hidden = text_encoder.initialize_hidden_state()\n",
    "wrong_hidden = text_encoder.initialize_hidden_state()\n",
    "\n",
    "n_rnn = 50\n",
    "n_ckpt = 50\n",
    "\n",
    "for epoch in range(start_epoch, hparas['N_EPOCH']):\n",
    "    g_total_loss = 0\n",
    "    d_total_loss = 0\n",
    "    r_total_loss = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for idx, ((image_real, caption_real), image_wrong, caption_wrong) in enumerate(zip(dataset, dataset_wrong_image, dataset_wrong_caption)):\n",
    "        #caption_real = tf.reshape(caption_real, [-1])\n",
    "        #caption_wrong = tf.reshape(caption_wrong, [-1])\n",
    "        \n",
    "        if epoch < n_rnn:\n",
    "            r_loss = train_step_rnn(image_real, caption_real, hidden, image_wrong, caption_wrong, wrong_hidden)\n",
    "            r_total_loss += r_loss\n",
    "        else:\n",
    "            g_loss, d_loss = train_step(image_real, caption_real, hidden, image_wrong, caption_wrong, wrong_hidden)\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "\n",
    "    time_tuple = time.localtime()\n",
    "    time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "\n",
    "    if epoch < n_rnn:\n",
    "        print(\"Epoch {}, rnn_loss: {:.4f}\".format(epoch+1, r_total_loss/steps_per_epoch))\n",
    "    else:\n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        \n",
    "    print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "\n",
    "    # save the model\n",
    "    if (epoch + 1) % n_ckpt == 0:\n",
    "        manager.save(checkpoint_number=((epoch + 1) // n_ckpt))\n",
    "\n",
    "    # visualization\n",
    "    if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "        for caption in sample_sentence:\n",
    "            fake_image = test_step(caption, sample_seed, hidden)\n",
    "        save_images(fake_image, [ni, ni],  SAMPLE_DIR + 'train_{:02d}.jpg'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    data = pd.read_pickle(PATH_TO_DATA + 'dataset/dataset/testData.pkl')\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(PATH_TO_DATA + 'dataset/dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave(IFERENCE_PATH + 'inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy())\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume training from epoch 850\n"
     ]
    }
   ],
   "source": [
    "# checkpoint.restore(checkpoint_dir + '/ckpt-1')\n",
    "\n",
    "selected_ckpt = CKPT_DIR + \"final-17\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                           discriminator_optimizer=discriminator_optimizer,\n",
    "                           rnn_optimizer=rnn_optimizer,\n",
    "                           text_encoder=text_encoder,\n",
    "                           cnn_encoder=cnn_encoder,\n",
    "                           generator=generator,\n",
    "                           discriminator=discriminator)\n",
    "\n",
    "ckpt.restore(selected_ckpt)\n",
    "start_epoch = int(selected_ckpt.split(\"-\")[-1]) * 50\n",
    "\n",
    "print(f'Resume training from epoch {start_epoch}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 4.4997 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_is_WG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
