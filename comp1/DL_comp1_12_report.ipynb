{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度學習第一次競賽報告 - 第12組 工具人智慧\n",
    "組員：\n",
    "<br/>107024501 高瑀鍹\n",
    "<br/>107024506 王子誠\n",
    "<br/>107024511 羅揚\n",
    "<br/>107024522 戴子翔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import string\n",
    "\n",
    "import gc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression, RidgeClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from  xgboost import XGBClassifier\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    # current\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # '[a-zA-Z]+' 至少有一個英文字母\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip()) if re.match('[a-zA-Z]+', word)]\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def preprocessor_ta(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./data/original/train.csv\")\n",
    "df_train.drop([\"Id\"],axis=1,inplace=True)\n",
    "\n",
    "df_test = pd.read_csv(\"./data/original/test.csv\")\n",
    "df_test.drop([\"Id\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('./data/original/train.csv')\n",
    "df_test_temp = pd.read_csv('./data/original/test.csv')\n",
    "\n",
    "X_train = df_temp['Page content']\n",
    "y_train = df_temp['Popularity']\n",
    "X_test = df_test_temp['Page content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text Data\n",
    "\n",
    "首先，我們先將文字分成標題 (title)、內容 (content)、主題 (topics)，因為三者在文字中的重要性不同，因此先將三者分開。\n",
    "<br/> 獲得文字後，我將文字從html格式取出、換成小寫、去除標點符號與stopword。此外，我發現有部分文章中會出現超連結，這些超連結轉為文字資料後意義不大，因此我也將這些連結移除。\n",
    "\n",
    "根據觀察，部分文章的內容經過前處理後，會出現如script、js、id、src等無意義詞彙，這是因為文字內出現以&lt;script&gt;為開頭的javascript程式碼，但此程式碼並不會被BeautifulSoup刪除，因此必須額外處理。\n",
    "<br/> 有趣的是，我發現開頭為&lt;script ...&gt;的javascript程式碼會被BeautifulSoup刪除，因此不須額外處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPoisition(text,pattern):\n",
    "    output=[]\n",
    "    p = re.compile(pattern)\n",
    "    temp = list()\n",
    "    for m in p.finditer(text):\n",
    "        temp.append(m.start())\n",
    "        \n",
    "    return temp\n",
    "\n",
    "def get_article_content(text):\n",
    "    tpoi = getPoisition(text, '<section class=\"')\n",
    "    text = text[tpoi[0]+len('<section class=\"'):]\n",
    "    \n",
    "    tpoi = getPoisition(text, '\"')\n",
    "    text = text[tpoi[0]+2:]\n",
    "    \n",
    "    tpoi = getPoisition(text,\"</article>\")\n",
    "\n",
    "    return text[:tpoi[0]]\n",
    "\n",
    "def get_title(text):\n",
    "    tpoi = getPoisition(text, '<h1 class=\"title\">')\n",
    "    text = text[tpoi[0]+len('<h1 class=\"title\">'):]\n",
    "    \n",
    "    tpoi = getPoisition(text, '</h1>')\n",
    "    text = text[:tpoi[0]]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_foot_topic(text, isN=False):\n",
    "    tpoi = getPoisition(text, '<footer class=\"article-topics\">')\n",
    "    tpoi2 = getPoisition(text, '</footer>')\n",
    "\n",
    "    footer_text = text[tpoi[0]+len('<footer class=\"article-topics\">'):tpoi2[0]]\n",
    "\n",
    "    prefix = getPoisition(footer_text, '/\">')\n",
    "    suffix = getPoisition(footer_text, '</a>')\n",
    "\n",
    "    output = []\n",
    "    for i in range(len(prefix)):\n",
    "        output.append(footer_text[prefix[i]+3:suffix[i]])\n",
    "        \n",
    "    if isN:\n",
    "        return len(output)\n",
    "    else:\n",
    "        return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    tpoi_http = getPoisition(text,\"http:\")\n",
    "    tpoi_https = getPoisition(text,\"https:\")\n",
    "\n",
    "    url_all = tpoi_http+tpoi_https\n",
    "    url_all.sort()\n",
    "\n",
    "    url_space_all = [0]\n",
    "\n",
    "    for url_poi in url_all:\n",
    "        tpoi = getPoisition(text[url_poi:],\" \")\n",
    "        url_space_all.append(url_poi+tpoi[0])\n",
    "\n",
    "    url_all.append(len(text))\n",
    "\n",
    "    output = []\n",
    "    for i in range(len(url_space_all)):\n",
    "        sub_text = text[url_space_all[i]:url_all[i]]\n",
    "        output.append(sub_text)\n",
    "        \n",
    "    return \" \".join(output)\n",
    "\n",
    "def preprocessing(text,isbs4=False):\n",
    "    if isbs4:\n",
    "        text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "        text = remove_url(text)\n",
    "    \n",
    "    text = re.sub('[^\\w\\s]', ' ', text)\n",
    "    text = re.sub('(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\\\s+', ' ', text)\n",
    "    text = \" \".join([x for x in text.split() if x not in stop])\n",
    "    text = \" \".join([x for x in text.split() if not x.isdigit()])\n",
    "\n",
    "    letters = [x for x in string.ascii_lowercase]\n",
    "    text = \" \".join([x for x in text.split() if x not in letters])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_js(text):\n",
    "    tpoi = getPoisition(text,'<script>')\n",
    "    \n",
    "    if len(tpoi)==0:\n",
    "        return text\n",
    "    \n",
    "    sFinish = [0]\n",
    "    for i in tpoi:\n",
    "        subpoi = getPoisition(text[i:],'</script>')\n",
    "        sFinish.append(i+subpoi[0]+len(\"</script>\"))\n",
    "\n",
    "    tpoi.append(len(text))\n",
    "\n",
    "    output = []\n",
    "    for i in range(len(tpoi)):\n",
    "        sub_text = text[sFinish[i]:tpoi[i]]\n",
    "        output.append(sub_text)\n",
    "\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "我接著在文字資料上定義了許多不同的Feature。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data channel、data type、日期\n",
    "我定義了data channel，因為這表示文章在網站中被分類的主題，能夠代表文章的方向。我也定義了article type，這能夠代表文章的類型。\n",
    "<br/>我也有將時間抓下，用以進行之後的處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_channel(text):\n",
    "    dc_poi = getPoisition(text, \"data-channel\")\n",
    "    text = text[dc_poi[0]:]\n",
    "    \n",
    "    g_poi = getPoisition(text, '\"')\n",
    "    output = text[g_poi[0]+1:g_poi[1]]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def get_article_type(text):\n",
    "    tpoi = getPoisition(text, '<section class=\"')\n",
    "    text = text[tpoi[0]+len('<section class=\"'):]\n",
    "    \n",
    "    tpoi = getPoisition(text, '\"')\n",
    "    \n",
    "    return text[:tpoi[0]]\n",
    "\n",
    "def get_date(text):\n",
    "    tpoi = getPoisition(text, '<time datetime=')\n",
    "    \n",
    "    if len(tpoi)==0:\n",
    "        return np.nan\n",
    "    \n",
    "    text = text[tpoi[0]+49:]\n",
    "\n",
    "    tpoi = getPoisition(text, ' ')\n",
    "    text = text[:tpoi[1]]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### html架構\n",
    "我也有針對html檔的架構定義Feature，包含p、div、h1、h2、js，因為這些東西可能涵蓋不同的段落、小標等資訊。若一個版面段落沒有分好，會影響到讀者看這篇文章的心情，因此我們加入一些和文章結構有關的變數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_p(text):\n",
    "    tpoi = getPoisition(text,\"<p>\")\n",
    "    return len(tpoi)\n",
    "\n",
    "def n_div(text):\n",
    "    tpoi = getPoisition(text,\"<div\")\n",
    "    return len(tpoi)\n",
    "\n",
    "def n_h1(text):\n",
    "    tpoi = getPoisition(text,\"<h1\")\n",
    "    return len(tpoi)\n",
    "\n",
    "def n_h2(text):\n",
    "    tpoi = getPoisition(text,\"<h2\")\n",
    "    return len(tpoi)\n",
    "\n",
    "def n_js(text):\n",
    "    tpoi = getPoisition(text,'</script>')\n",
    "    return len(tpoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章內容中的資訊\n",
    "我也定義了一些可能與文章內容有關的Feature，如圖片、youtube、twitter、社群網路、連結、see also個數。我認為這些Feature可能能夠包含一些文章的內容架構。\n",
    "<br/>此外，我在處理主題文字時，我發現有部分主題可能包含不只一個字，因此我認為將主題個數納入考量也是重要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_link(text):\n",
    "    htt = getPoisition(text,'href=\"http:')\n",
    "    htts = getPoisition(text,'href=\"https:')\n",
    "    \n",
    "    return len(htts)+len(htt)\n",
    "\n",
    "def n_image(text,isClass=True):\n",
    "    if isClass:\n",
    "        tpoi = getPoisition(text, 'class=\"image\"')\n",
    "        \n",
    "        return len(tpoi)\n",
    "    else:    \n",
    "        tpoi_jpg = getPoisition(text, '.jpg')\n",
    "        tpoi_png = getPoisition(text, '.png')\n",
    "        tpoi_jpeg = getPoisition(text, '.jpeg')\n",
    "\n",
    "        return len(tpoi_jpg)+len(tpoi_png)+len(tpoi_jpeg)\n",
    "\n",
    "def n_youtube(text):\n",
    "    # video\n",
    "    tpoi = getPoisition(text, 'www.youtube.com')\n",
    "    return len(tpoi)\n",
    "\n",
    "def n_twitter(text):\n",
    "    # tweets\n",
    "    tpoi = getPoisition(text,'class=\"twitter-tweet')\n",
    "    return len(tpoi)\n",
    "\n",
    "def n_social_media(text):\n",
    "    # all kind of data\n",
    "    tpoi_yt = getPoisition(text, 'youtube.com')\n",
    "    tpoi_twitter =getPoisition(text,'twitter.com')\n",
    "    tpoi_fb =getPoisition(text,'facebook.com')\n",
    "    \n",
    "    return len(tpoi_yt)+len(tpoi_twitter)+len(tpoi_fb)\n",
    "\n",
    "def n_see_also(text):\n",
    "    tpoi_upper = getPoisition(text,\"SEE ALSO\")\n",
    "    tpoi_lower = getPoisition(text,\"see also\")\n",
    "    tpoi_mix1 = getPoisition(text,\"See also\")\n",
    "    tpoi_mix2 = getPoisition(text,\"See Also\")\n",
    "    \n",
    "    return len(tpoi_upper)+len(tpoi_lower)+len(tpoi_mix1)+len(tpoi_mix2)\n",
    "\n",
    "def out_url(text,rType):  \n",
    "    tpoi_start = getPoisition(text,\"<ul>\")\n",
    "    tpoi_end = getPoisition(text,\"</ul>\")\n",
    "    \n",
    "    if rType==\"n_ul\":\n",
    "        return len(tpoi_start)\n",
    "    \n",
    "    elif rType == \"n_link\":\n",
    "        total_url = 0\n",
    "        \n",
    "        for i in range(len(tpoi_start)):\n",
    "            total_url += n_link(text[tpoi_start[i]:tpoi_end[i]])\n",
    "    \n",
    "        return total_url\n",
    "    \n",
    "    elif rType==\"n_token\":\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    df[\"content\"] = df[\"Page content\"].apply(lambda x:preprocessing(remove_js(get_article_content(x)),isbs4=True))\n",
    "    df[\"title\"] = df[\"Page content\"].apply(lambda x:preprocessing(get_title(x),isbs4=False))\n",
    "    df[\"topics\"] = df[\"Page content\"].apply(lambda x:preprocessing(get_foot_topic(x,isN=False),isbs4=False))\n",
    "\n",
    "    df[\"n_topics\"] = df[\"Page content\"].apply(lambda x:get_foot_topic(x,isN=True))\n",
    "\n",
    "    df[\"data_channel\"] = df[\"Page content\"].apply(lambda x:get_data_channel(x))\n",
    "    df[\"article_type\"] = df[\"Page content\"].apply(lambda x:get_article_type(x))\n",
    "\n",
    "    df[\"datetime\"] = df[\"Page content\"].apply(lambda x:get_date(x))\n",
    "    df[\"n_link_all\"]=df[\"Page content\"].apply(lambda x:n_link(remove_js(x)))\n",
    "    df[\"n_link_content\"]=df[\"Page content\"].apply(lambda x:n_link(remove_js(get_article_content(x))))\n",
    "    df[\"n_js_all\"]=df[\"Page content\"].apply(lambda x:n_js(x))\n",
    "    df[\"n_js_content\"]=df[\"Page content\"].apply(lambda x:n_js(get_article_content(x)))\n",
    "\n",
    "    df[\"n_image\"] = df[\"Page content\"].apply(lambda x:n_image(x,isClass=True))\n",
    "    df[\"n_youtube\"] = df[\"Page content\"].apply(lambda x:n_youtube(x))\n",
    "    df[\"n_twitter\"] = df[\"Page content\"].apply(lambda x:n_twitter(x))\n",
    "    df[\"n_social_media\"] = df[\"Page content\"].apply(lambda x:n_social_media(x))\n",
    "\n",
    "    df[\"n_p\"] = df[\"Page content\"].apply(lambda x:n_p(get_article_content(x)))\n",
    "    df[\"n_div\"] = df[\"Page content\"].apply(lambda x:n_div(get_article_content(x)))\n",
    "    df[\"n_h1\"] = df[\"Page content\"].apply(lambda x:n_h1(get_article_content(x)))\n",
    "    df[\"n_h2\"] = df[\"Page content\"].apply(lambda x:n_h2(get_article_content(x)))\n",
    "\n",
    "    df[\"n_see_also\"] = df[\"Page content\"].apply(lambda x:n_see_also(x))\n",
    "    df[\"n_ul\"] = df[\"Page content\"].apply(lambda x:out_url(get_article_content(x),rType=\"n_ul\"))\n",
    "    df[\"n_ul_link\"] = df[\"Page content\"].apply(lambda x:out_url(get_article_content(x),rType=\"n_link\"))\n",
    "\n",
    "    df.drop([\"Page content\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標題、內容、標題+內容的字數\n",
    "我首先定義了標題、內容、標題+內容的長度、字數、每個字的長度，我認為前兩者可以代表文章本身的長短，每個字的長度則可測量作者的寫作能力。我接著定義了標題與標題+內容在長度、字數、每個字的長度上的比例，用以標示標題在整個頁面中的分量。\n",
    "<br/>另外，我也定義了內容中是否有出現breaking news的字樣，因為breaking news通常會是重點新聞，可能會影響文章熱度。\n",
    "<br/>我接著定義了標題字數、長度以及每個標題的平均字數與長度。這是用來標示標題的文字多寡，我認為者可能與作者是否花心思寫有關，進而影響到人氣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_breaking_news(text):\n",
    "    if \"breaking news\" in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"title_len\"] = df[\"title\"].apply(lambda x: len(x))\n",
    "    df[\"title_n_item\"] = df[\"title\"].apply(lambda x: len(tokenizer(x)))\n",
    "    df[\"title_avg_len\"] = df[\"title_len\"]/df[\"title_n_item\"]\n",
    "\n",
    "    df[\"content_len\"] = df[\"content\"].apply(lambda x: len(x))\n",
    "    df[\"content_n_item\"] = df[\"content\"].apply(lambda x: len(tokenizer(x)))\n",
    "    df[\"content_avg_len\"] = df[\"content_len\"]/df[\"content_n_item\"]\n",
    "\n",
    "    df[\"page_len\"] = df[\"title_len\"] +df[\"content_len\"]\n",
    "    df[\"page_n_item\"] = df[\"title_n_item\"]+df[\"content_n_item\"]\n",
    "    df[\"page_avg_len\"] = df[\"page_len\"]/df[\"page_n_item\"]\n",
    "\n",
    "    df[\"title_len_ratio\"] = df[\"page_len\"]/df[\"title_len\"]\n",
    "    df[\"title_n_item_ratio\"] = df[\"page_n_item\"]/df[\"title_n_item\"]\n",
    "    df[\"title_avg_len_ratio\"] = df[\"page_avg_len\"]/df[\"title_avg_len\"]\n",
    "\n",
    "    df[\"topics_n_word\"] = df[\"topics\"].apply(lambda x: len(x.split()))\n",
    "    df[\"topics_len\"] = df[\"topics\"].apply(lambda x: len(x.replace(\" \",\"\")))\n",
    "\n",
    "    df[\"topics_avg_word\"] = df[\"topics_n_word\"]/df[\"n_topics\"]\n",
    "    df[\"topics_avg_len\"] = df[\"topics_len\"]/df[\"n_topics\"]\n",
    "\n",
    "    df[\"topics_avg_word\"].fillna(0,inplace=True)\n",
    "    df[\"topics_avg_len\"].fillna(0,inplace=True)\n",
    "\n",
    "    df[\"is_breaking_news\"] = df[\"content\"].apply(lambda x: is_breaking_news(x))\n",
    "    df[\"link_per_ul\"] = df[\"n_ul_link\"]/df[\"n_ul\"]\n",
    "    df[\"link_per_ul\"].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 計算文章字數、標題字數等資訊\n",
    "## 不同組員的定義程式碼\n",
    "\n",
    "word = []; word_test = []\n",
    "for i in range(len(X_train)): \n",
    "    soup = BeautifulSoup(X_train.iloc[i],'html.parser')\n",
    "    word.append(len(str(soup.prettify)))\n",
    "    \n",
    "for i in range(len(X_test)): \n",
    "    soup = BeautifulSoup(X_test.iloc[i],'html.parser')\n",
    "    word_test.append(len(str(soup.prettify)))\n",
    "    \n",
    "title = []; title_test = []\n",
    "for i in range(len(X_train)) : \n",
    "    soup = BeautifulSoup(X_train.iloc[i],'html.parser')\n",
    "    title.append(soup.h1.string)\n",
    "    \n",
    "for i in range(len(X_test)) : \n",
    "    soup = BeautifulSoup(X_test.iloc[i],'html.parser')\n",
    "    title_test.append(soup.h1.string)\n",
    "title1 = []; title1_test = []\n",
    "for i in range(len(X_train)):\n",
    "    title1.append(len(tokenizer_stem_nostop(preprocessor_ta(title[i]))))\n",
    "for i in range(len(X_test)):\n",
    "    title1_test.append(len(tokenizer_stem_nostop(preprocessor_ta(title_test[i]))))\n",
    "    \n",
    "link = []; link_test = []; p = []; p_test = []; video = []; video_test = []\n",
    "for i in range(len(X_train)) : \n",
    "    soup = BeautifulSoup(X_train.iloc[i],'html.parser')\n",
    "    link.append(len(soup.article.find_all(['a'])))\n",
    "    p.append(len(soup.article.find_all(['p'])))\n",
    "    video.append(len(soup.find_all(\"iframe\")))\n",
    "    \n",
    "for i in range(len(X_test)) : \n",
    "    soup = BeautifulSoup(X_test.iloc[i],'html.parser')\n",
    "    link_test.append(len(soup.article.find_all(['a'])))\n",
    "    p_test.append(len(soup.article.find_all(['p'])))\n",
    "    video_test.append(len(soup.find_all(\"iframe\")))\n",
    "    \n",
    "X = pd.concat([X_train,X_test],axis = 0)\n",
    "Article = []\n",
    "for i in range(len(X)) : \n",
    "    soup = BeautifulSoup(X.iloc[i],'html.parser')\n",
    "    Article.append(soup.article.attrs['data-channel'])\n",
    "Article = pd.get_dummies(Article)\n",
    "article = Article.iloc[range(len(X_train)),:]\n",
    "article_test = Article.iloc[range(len(X_train),len(X),1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時間相關變數\n",
    "對於時間方面，我找出文章的年、月、日、星期幾、是否為週末、小時、是否為早午晚凌晨、文章發布日至指定日期的天數。\n",
    "<br/>因為文章所發布的熱度很有可能與時間背景或是不同時間的上網人數有關，因此時間變數可能影響人氣。\n",
    "<br/>此外，因為testing data中存在一筆時間為missing，因此我將文章發布日至指定日期的天數使用平均值補值，並以此推算年、月、日、星期幾、是否為週末。小時等資訊則用平均值補值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_back_up_to_now(df,index_m):\n",
    "    df.loc[index_m,\"up_to_now\"] = df.loc[~index_m,\"up_to_now\"].mean().round()\n",
    "    df.loc[index_m,\"hour\"] = df.loc[~index_m,\"hour\"].mean().round()\n",
    "    df.loc[index_m,\"datetime\"] =  datetime.datetime(2019,10,18)-datetime.timedelta(days=int(df.loc[index_m,\"up_to_now\"]))\n",
    "    \n",
    "    df.loc[index_m,\"year\"] = df.loc[index_m,\"datetime\"].dt.year\n",
    "    df.loc[index_m,\"month\"] = df.loc[index_m,\"datetime\"].dt.month\n",
    "    df.loc[index_m,\"day\"] = df.loc[index_m,\"datetime\"].dt.day\n",
    "    df.loc[index_m,\"dayofweek\"] = df.loc[index_m,\"datetime\"].dt.dayofweek+1\n",
    "    df.loc[index_m,\"weekend\"]=(df.loc[index_m,\"dayofweek\"]>=6).astype(int)\n",
    "    \n",
    "    df.loc[index_m,\"is_night\"] = (df.loc[index_m,'hour']<=6).astype(int)\n",
    "    df.loc[index_m,\"is_morning\"] = ((df.loc[index_m,'hour']>6) & (df.loc[index_m,'hour']<=12)).astype(int)\n",
    "    df.loc[index_m,\"is_afternoon\"] = ((df.loc[index_m,'hour']>12) & (df.loc[index_m,'hour']<=18)).astype(int)\n",
    "    df.loc[index_m,\"is_evening\"] = (df.loc[index_m,'hour']>18).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "is_train = True\n",
    "for df in [df_train, df_test]:\n",
    "    if is_train:\n",
    "        df[\"datetime\"]= pd.to_datetime(df[\"datetime\"])\n",
    "    else:\n",
    "        dt_missing = df[\"datetime\"].isna()\n",
    "        df[\"datetime\"]= pd.to_datetime(df[\"datetime\"])\n",
    "        df[\"datetime\"] = df[\"datetime\"].fillna(datetime.datetime(2018,12,31))\n",
    "\n",
    "    df[\"year\"] = df[\"datetime\"].dt.year\n",
    "    df[\"month\"] = df[\"datetime\"].dt.month\n",
    "    df[\"day\"] = df[\"datetime\"].dt.day\n",
    "    df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek+1\n",
    "    df[\"weekend\"]=(df[\"dayofweek\"]>=6).astype(int)\n",
    "\n",
    "    df['hour'] = df[\"datetime\"].dt.hour\n",
    "\n",
    "    df[\"is_night\"] = (df['hour']<=6).astype(int)\n",
    "    df[\"is_morning\"] = ((df['hour']>6) & (df['hour']<=12)).astype(int)\n",
    "    df[\"is_afternoon\"] = ((df['hour']>12) & (df['hour']<=18)).astype(int)\n",
    "    df[\"is_evening\"] = (df['hour']>18).astype(int)\n",
    "\n",
    "    df[\"up_to_now\"]=(datetime.datetime(2019,10,18)-df[\"datetime\"]).dt.days\n",
    "\n",
    "    if not is_train:\n",
    "        df = cal_back_up_to_now(df,dt_missing)\n",
    "        \n",
    "    is_train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章架構有關變數及交互作用項\n",
    "我接著定義了許多跟文章架構有關的變數以及交互作用項。\n",
    "<br/>如平均每段的字數、平均每段的社群媒體、h1標題的比例、h2標題的比例，我認為這些都可能影響到讀者對於文章架構的觀感，進而影響人氣。我也定義了文章長度與段落數、社群媒體數與段落數的交互作用，因為我認為不同的段落數下，文章的長短、社群媒體數的影響可能不同。\n",
    "<br/>我額外定義一個時間變數為是否為工作時間，並計算是否為工作時間與星期的交互作用，小時與星期、是否為工作時間的交互作用，因為我認為不同的星期在不同的時間行為應該不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_test]:\n",
    "    df[\"content_per_p\"] =df[\"content_len\"] / df[\"n_p\"]\n",
    "    df[\"n_h\"] = df[\"n_h1\"]+df[\"n_h2\"]\n",
    "    df[\"n_h1_ratio\"] = df[\"n_h1\"]/df[\"n_h\"]\n",
    "    df[\"n_h2_ratio\"] = df[\"n_h2\"]/df[\"n_h\"]\n",
    "\n",
    "    df[\"n_h1_ratio\"].fillna(0,inplace=True)\n",
    "    df[\"n_h2_ratio\"].fillna(0,inplace=True)\n",
    "\n",
    "    df[\"is_working\"] = ((df['hour']>=10) & (df['hour']<=18)).astype(int)\n",
    "    df[\"is_working_dayofweek\"] = df[\"is_working\"]*df[\"dayofweek\"]\n",
    "    df[\"hour_dayofweek\"] = df[\"hour\"]*df[\"dayofweek\"]\n",
    "    df[\"is_working_hour\"] = df[\"hour\"]*df[\"is_working\"]\n",
    "\n",
    "    df[\"content_per_p\"] = df[\"content_len\"]/df[\"n_p\"]\n",
    "    df[\"content_p_inter\"] = df[\"content_len\"]*df[\"n_p\"]\n",
    "\n",
    "    df[\"p_per_social_media\"] = df[\"n_social_media\"]/df[\"n_p\"]\n",
    "    df[\"p_inter_social_media\"] = df[\"n_social_media\"]*df[\"n_p\"]\n",
    "\n",
    "    df[\"content_social_inter\"] = df[\"n_social_media\"]/(df[\"content_len\"]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data channel的轉換\n",
    "我發現有些data channel的數量十分稀少，這些data channel應該可以透過適當的轉換與其他合併。\n",
    "<br/>有些data channel如bus、mob、howto、socmed可以從字面上判斷出其為某些data channel的所寫，因此可以很簡單的找到其應該合併的對象。\n",
    "<br/>至於其他data channel則是我透過一篇一篇慢慢檢視來找出其可能轉換的對象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_test]:\n",
    "    small_set = set([\"comics\",\"memes\",\"sports\",\"jobs\",\"home\",\"viral\",\"travel-leisure\",\"conversations\"])\n",
    "    df[\"data_channel_01\"] = df[\"data_channel\"].apply(lambda x: 1 if x in small_set else 0)\n",
    "\n",
    "    df[\"data_channel\"] = df[\"data_channel\"].replace({\"howto\":\"how-to\",\n",
    "                                                     \"mob\":\"mobile\",\n",
    "                                                     \"bus\":\"business\",\n",
    "                                                     \"socmed\":\"social-media\",\n",
    "                                                     \"comics\":\"lifestyle\",\n",
    "                                                     \"memes\":\"lifestyle\",\n",
    "                                                     \"sports\":\"lifestyle\",\n",
    "                                                     \"jobs\":\"business\",\n",
    "                                                     \"home\":\"lifestyle\",\n",
    "                                                     \"viral\":\"entertainment\",\n",
    "                                                     \"travel-leisure\":\"lifestyle\",\n",
    "                                                     \"conversations\":\"lifestyle\"\n",
    "                                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  內容與標題的關係\n",
    "<br/>我額外嘗試了其他的內容與標題的關係，包含兩者在長度、字數、平均長度上的比例，試圖捕捉兩者在呈現上的關係。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_6 = df_train.copy()\n",
    "df_test_6 = df_test.copy()\n",
    "\n",
    "for df in [df_train_6, df_test_6]:\n",
    "    df[\"content_title_len_ratio\"] = df[\"content_len\"]/df[\"title_len\"]\n",
    "    df[\"content_title_n_item_ratio\"] = df[\"content_n_item\"]/df[\"title_n_item\"]\n",
    "    df[\"content_title_avg_len_ratio\"] = df[\"content_avg_len\"]/df[\"title_avg_len\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 節日\n",
    "因為在節日的時候有可能會出現許多節日相關新聞，使其人氣較高，因此我定義出了Feature表示是否為美國的節日，其中再細分出帶薪休假。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirstmas = set([datetime.date(2013,12,25),datetime.date(2014,12,25),datetime.date(2015,12,25)])\n",
    "nyEve = set([datetime.date(2013,12,31),datetime.date(2014,12,31),datetime.date(2015,12,31)])\n",
    "thanksgiving = set([datetime.date(2013,11,28),datetime.date(2014,11,27),datetime.date(2015,11,26)])\n",
    "ny = set([datetime.date(2013,1,1),datetime.date(2014,1,1),datetime.date(2015,1,1)])\n",
    "motherDay = set([datetime.date(2013,5,12),datetime.date(2014,5,11),datetime.date(2015,5,10)])\n",
    "easter = set([datetime.date(2013,3,31),datetime.date(2014,4,20),datetime.date(2015,4,5)])\n",
    "independence = set([datetime.date(2013,7,4),datetime.date(2014,7,4),datetime.date(2015,7,4)])\n",
    "fatherDay = set([datetime.date(2013,6,16),datetime.date(2014,6,15),datetime.date(2015,6,21)])\n",
    "howlloween = set([datetime.date(2013,10,31),datetime.date(2014,10,31),datetime.date(2015,10,31)])\n",
    "valentine = set([datetime.date(2013,2,14),datetime.date(2014,2,14),datetime.date(2015,2,14)])\n",
    "saintPatrick = set([datetime.date(2013,3,17),datetime.date(2014,3,17),datetime.date(2015,3,17)])\n",
    "memorial = set([datetime.date(2013,5,27),datetime.date(2014,5,26),datetime.date(2015,5,25)])\n",
    "laborDay = set([datetime.date(2013,9,2),datetime.date(2014,9,1),datetime.date(2015,9,7)])\n",
    "\n",
    "paid_day = chirstmas.union(ny,memorial,independence,laborDay,thanksgiving,chirstmas)\n",
    "all_holiday = chirstmas.union(nyEve,thanksgiving,ny,motherDay,easter,independence,fatherDay,\n",
    "                              howlloween,valentine,saintPatrick,memorial,laborDay)\n",
    "\n",
    "for df in [df_train_6, df_test_6]:\n",
    "    df[\"chirstmas\"] = df[\"datetime\"].dt.date.apply(lambda x: x in chirstmas).astype(int)\n",
    "    df[\"thanksgiving\"] = df[\"datetime\"].dt.date.apply(lambda x: x in thanksgiving).astype(int)\n",
    "    df[\"motherDay\"] = df[\"datetime\"].dt.date.apply(lambda x: x in motherDay).astype(int)\n",
    "    df[\"easter\"] = df[\"datetime\"].dt.date.apply(lambda x: x in easter).astype(int)\n",
    "    df[\"independence\"] = df[\"datetime\"].dt.date.apply(lambda x: x in independence).astype(int)\n",
    "    df[\"fatherDay\"] = df[\"datetime\"].dt.date.apply(lambda x: x in fatherDay).astype(int)\n",
    "    df[\"howlloween\"] = df[\"datetime\"].dt.date.apply(lambda x: x in howlloween).astype(int)\n",
    "    df[\"valentine\"] = df[\"datetime\"].dt.date.apply(lambda x: x in valentine).astype(int)\n",
    "    df[\"saintPatrick\"] = df[\"datetime\"].dt.date.apply(lambda x: x in saintPatrick).astype(int)\n",
    "    df[\"memorial\"] = df[\"datetime\"].dt.date.apply(lambda x: x in memorial).astype(int)\n",
    "    df[\"laborDay\"] = df[\"datetime\"].dt.date.apply(lambda x: x in laborDay).astype(int)\n",
    "\n",
    "    df[\"paid_day\"] = df[\"datetime\"].dt.date.apply(lambda x: x in paid_day).astype(int)\n",
    "    df[\"all_holiday\"] = df[\"datetime\"].dt.date.apply(lambda x: x in all_holiday).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章時間至年底的間隔\n",
    "最後，我再定義了一個變數表示文章的時間至年底的間隔，因為我觀察到文章人氣的高低很有可能跟文章發表的日期是在一年中的哪天有關，因此加此變數捕捉此效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_8 = df_train_6.copy()\n",
    "df_test_8 = df_test_6.copy()\n",
    "\n",
    "for df in [df_train_8, df_test_8]:\n",
    "    df[\"up_to_EoY\"] = (df[\"year\"].apply(lambda x: datetime.datetime(x,12,31,23,59))-df[\"datetime\"]).dt.days\n",
    "    \n",
    "for df in [df_train, df_test,\n",
    "           df_train_6, df_test_6,\n",
    "           df_train_8, df_test_8]:\n",
    "    df.drop([\"datetime\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "data channel與article type為categorical data，必須對其進行轉換，因為我主要的方法為lightgbm，只需使用Label Encoding配上標示那些資料為類別資料即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train[\"Popularity\"]\n",
    "df_train.drop([\"Popularity\"],axis=1,inplace=True)\n",
    "df_train_6.drop([\"Popularity\"],axis=1,inplace=True)\n",
    "df_train_8.drop([\"Popularity\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tr = df_train.shape[0]\n",
    "cate_col = [\"data_channel\",\"article_type\"]\n",
    "\n",
    "##\n",
    "tr_te = pd.concat([df_train, df_test],axis=0)\n",
    "for col in cate_col:\n",
    "    lbl = LabelEncoder()\n",
    "    tr_te[col] = lbl.fit_transform(tr_te[col])\n",
    "\n",
    "df_train = tr_te.iloc[:n_tr,:]\n",
    "df_test = tr_te.iloc[n_tr:,:]\n",
    "\n",
    "##\n",
    "tr_te = pd.concat([df_train_6, df_test_6],axis=0)\n",
    "for col in cate_col:\n",
    "    lbl = LabelEncoder()\n",
    "    tr_te[col] = lbl.fit_transform(tr_te[col])\n",
    "\n",
    "df_train_6 = tr_te.iloc[:n_tr,:]\n",
    "df_test_6 = tr_te.iloc[n_tr:,:]\n",
    "\n",
    "##\n",
    "tr_te = pd.concat([df_train_8, df_test_8],axis=0)\n",
    "for col in cate_col:\n",
    "    lbl = LabelEncoder()\n",
    "    tr_te[col] = lbl.fit_transform(tr_te[col])\n",
    "\n",
    "df_train_8 = tr_te.iloc[:n_tr,:]\n",
    "df_test_8 = tr_te.iloc[n_tr:,:]\n",
    "\n",
    "del tr_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將時間變數設為Dummy variable\n",
    "我們另外嘗試過將時間變數使用dummy variable的方式進行處理，配上XGBoost可以獲得不錯的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = []; ti_test = []; week = []; week_test = [] \n",
    "for i in range(len(X_train)) : \n",
    "    soup = BeautifulSoup(X_train.iloc[i],'html.parser')\n",
    "    week.append(soup.time.attrs['datetime'][0:3])\n",
    "    ti.append(soup.time.string)    \n",
    "for i in range(1585) :\n",
    "    soup_test = BeautifulSoup(X_test.iloc[i],'html.parser')\n",
    "    week_test.append(soup_test.time.attrs['datetime'][0:3])\n",
    "    ti_test.append(soup_test.time.string) \n",
    "for i in range(1586,len(X_test),1) :\n",
    "    soup_test = BeautifulSoup(X_test.iloc[i],'html.parser')\n",
    "    week_test.append(soup_test.time.attrs['datetime'][0:3])\n",
    "    ti_test.append(soup_test.time.string) \n",
    "temp = week_test[1585:11846]\n",
    "temp_ti = ti_test[1585:11846]\n",
    "week_test[1585] = 'Mon'\n",
    "ti_test[1585] = '2013-06-19 15:04:30 UTC' \n",
    "week_test[1586:len(X_test)] =  temp\n",
    "ti_test[1586:len(X_test)] = temp_ti\n",
    "\n",
    "month = []; month_test = []; hour = []; hour_test = [];year = []; year_test = [];\n",
    "for i in range(len(X_train)) : \n",
    "    year.append(preprocessor_ta(ti[i])[0:4])\n",
    "    month.append(preprocessor_ta(ti[i])[5:7])\n",
    "    hour.append(preprocessor_ta(ti[i])[11:13])\n",
    "\n",
    "for i in range(len(X_test)) :  \n",
    "    year_test.append(preprocessor_ta(ti_test[i])[0:4])\n",
    "    month_test.append(preprocessor_ta(ti_test[i])[5:7])    \n",
    "    hour_test.append(preprocessor_ta(ti_test[i])[11:13])\n",
    "year[27314] = '2013'\n",
    "\n",
    "day = [];day_test = []; sec = []; sec_test = []\n",
    "for i in range(len(X_train)) : \n",
    "    day.append(preprocessor_ta(ti[i])[8:10])\n",
    "    sec.append(preprocessor_ta(ti[i])[14:16])\n",
    "for i in range(len(X_test)) :  \n",
    "    day_test.append(preprocessor_ta(ti_test[i])[8:10])\n",
    "    sec_test.append(preprocessor_ta(ti_test[i])[14:16])\n",
    "    \n",
    "week = pd.get_dummies(week)\n",
    "week_test = pd.get_dummies(week_test)\n",
    "month = pd.get_dummies(month)\n",
    "month_test = pd.get_dummies(month_test)\n",
    "hour = pd.get_dummies(hour)\n",
    "hour_test = pd.get_dummies(hour_test)\n",
    "year = pd.get_dummies(year)\n",
    "year_test = pd.get_dummies(year_test)\n",
    "day1 = pd.get_dummies(day)\n",
    "day1_test = pd.get_dummies(day_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時間dummy variable的交互作用項\n",
    "這些變數間因為都是有或沒有(1 or 0)，故我們去對他們做相乘會得到兩個的交集會是1，那這樣也會是個很有幫助的變數，例如時間變數的相乘，就可以得到更詳細的時間變數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YA = pd.DataFrame(); YA_test = pd.DataFrame()\n",
    "for i in range(len(year.columns)):\n",
    "    for j in range(len(article.columns)):\n",
    "        YA = pd.concat([YA,year.iloc[:,i]*article.iloc[:,j]],axis = 1)\n",
    "        YA_test = pd.concat([YA_test,year_test.iloc[:,i]*article_test.iloc[:,j]],axis = 1)\n",
    "MY = pd.DataFrame(); MY_test = pd.DataFrame()\n",
    "for i in range(len(month.columns)):\n",
    "    for j in range(len(year.columns)):\n",
    "        MY = pd.concat([MY,month.iloc[:,i]*year.iloc[:,j]],axis = 1)\n",
    "        MY_test = pd.concat([MY_test,month_test.iloc[:,i]*year_test.iloc[:,j]],axis = 1)\n",
    "WY = pd.DataFrame(); WY_test = pd.DataFrame()\n",
    "for i in range(len(week.columns)):\n",
    "    for j in range(len(year.columns)):\n",
    "        WY = pd.concat([WY,week.iloc[:,i]*year.iloc[:,j]],axis = 1)\n",
    "        WY_test = pd.concat([WY_test,week_test.iloc[:,i]*year_test.iloc[:,j]],axis = 1)\n",
    "AP = pd.DataFrame(); AP_test = pd.DataFrame()\n",
    "for i in range(len(year.columns)):\n",
    "    AP = pd.concat([AP,article.iloc[:,i]*np.abs(pol)],axis = 1)\n",
    "    AP_test = pd.concat([AP_test,article_test.iloc[:,i]*np.abs(pol_test)],axis = 1)\n",
    "DY = pd.DataFrame(); DY_test = pd.DataFrame()\n",
    "for i in range(len(day1.columns)):\n",
    "    for j in range(len(year.columns)):\n",
    "        DY = pd.concat([DY,day1.iloc[:,i]*year.iloc[:,j]],axis = 1)\n",
    "        DY_test = pd.concat([DY_test,day1_test.iloc[:,i]*year_test.iloc[:,j]],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Extraction\n",
    "我使用數種方法進行Text Feature Extraction，包含TF-IDF、TFIDF+SVD、TF+主題模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF+SVD (Method 1)\n",
    "我將內容、標題、主題分別計算TF-IDF。因為維度過高，我使用SVD分解 (即LSA) 進行降維，維度個數根據變異解釋比例決定，分別選到1000、1000、500。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## content\n",
    "content_tfidf_18 = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                min_df=20,\n",
    "                                max_df=0.4,\n",
    "                                tokenizer=tokenizer_stem)\n",
    "\n",
    "content_tr_tf_18 = content_tfidf_18.fit_transform(df_train[\"content\"])\n",
    "content_te_tf_18 =content_tfidf_18.transform(df_test[\"content\"])\n",
    "\n",
    "## title\n",
    "title_tfidf_18 = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                              min_df=2,\n",
    "                              max_df=0.95,\n",
    "                              tokenizer=tokenizer_stem)\n",
    "\n",
    "title_tr_tf_18 = title_tfidf_18.fit_transform(df_train[\"title\"])\n",
    "title_te_tf_18 = title_tfidf_18.transform(df_test[\"title\"])\n",
    "\n",
    "## topics\n",
    "topics_tfidf_18 = TfidfVectorizer(ngram_range=(1,1),\n",
    "                               min_df=2,\n",
    "                               tokenizer=tokenizer_stem)\n",
    "\n",
    "topics_tr_tf_18 = topics_tfidf_18.fit_transform(df_train[\"topics\"])\n",
    "topics_te_tf_18 = topics_tfidf_18.transform(df_test[\"topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## content\n",
    "svder = TruncatedSVD(n_components=1000, n_iter=10, random_state=4342)\n",
    "svder.fit(content_tr_tf_18)\n",
    "\n",
    "content_svd_tr_18 = svder.transform(content_tr_tf_18)\n",
    "content_svd_te_18 = svder.transform(content_te_tf_18)\n",
    "\n",
    "## title\n",
    "svder = TruncatedSVD(n_components=1000, n_iter=10, random_state=2342)\n",
    "svder.fit(title_tr_tf_18)\n",
    "\n",
    "title_svd_tr_18 = svder.transform(title_tr_tf_18)\n",
    "title_svd_te_18 = svder.transform(title_te_tf_18)\n",
    "\n",
    "## topics\n",
    "svder = TruncatedSVD(n_components=500, n_iter=10, random_state=4352)\n",
    "svder.fit(topics_tr_tf_18)\n",
    "\n",
    "topics_svd_tr_18 = svder.transform(topics_tr_tf_18)\n",
    "topics_svd_te_18 = svder.transform(topics_te_tf_18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title+Topics (Method 2)\n",
    "因為內容資料眾多，因此我嘗試只使用標題與主題並各別計算TF-IDF，因為這兩個能夠有效率的代表文章本身，其相較文章內容更為精鍊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## title\n",
    "title_tfidf_22 = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                              min_df=3,\n",
    "                              max_df=0.95,\n",
    "                              tokenizer=tokenizer_stem)\n",
    "\n",
    "title_tr_tf_22 = title_tfidf_22.fit_transform(df_train_6[\"title\"])\n",
    "title_te_tf_22 = title_tfidf_22.transform(df_test_6[\"title\"])\n",
    "\n",
    "title_feature_name_22 = [\"_title_\"+i for i in title_tfidf_22.get_feature_names()]\n",
    "\n",
    "## topics\n",
    "topics_tfidf_22 = TfidfVectorizer(ngram_range=(1,1),\n",
    "                               min_df=2,\n",
    "                               tokenizer=tokenizer_stem)\n",
    "\n",
    "topics_tr_tf_22 = topics_tfidf_22.fit_transform(df_train_6[\"topics\"])\n",
    "topics_te_tf_22 = topics_tfidf_22.transform(df_test_6[\"topics\"])\n",
    "\n",
    "topics_feature_name_22 = [\"_topics_\"+i for i in topics_tfidf_22.get_feature_names()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation1 (Method 3)\n",
    "因為文章的主題很有可能影響文章的人氣，相較於單純使用文章的標籤主題，我嘗試使用Topics model來訓練出整個頁面文字形成的主題。\n",
    "<br/>我先嘗試對標題、內容、主題各自使用LDA，主題個數皆為100。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## content\n",
    "content_tfidf_26 = CountVectorizer(ngram_range=(1, 1),\n",
    "                                min_df=20,\n",
    "                                max_df=0.4,\n",
    "                                tokenizer=tokenizer_stem)\n",
    "\n",
    "content_tr_tf_26 = content_tfidf_26.fit_transform(df_train_6[\"content\"])\n",
    "content_te_tf_26 =content_tfidf_26.transform(df_test_6[\"content\"])\n",
    "\n",
    "## title\n",
    "title_tfidf_26 = CountVectorizer(ngram_range=(1, 3),\n",
    "                              min_df=20,\n",
    "                              tokenizer=tokenizer_stem)\n",
    "\n",
    "title_tr_tf_26 = title_tfidf_26.fit_transform(df_train_6[\"title\"])\n",
    "title_te_tf_26 = title_tfidf_26.transform(df_test_6[\"title\"])\n",
    "\n",
    "## topics\n",
    "topics_tfidf_26 = CountVectorizer(ngram_range=(1,2),\n",
    "                               min_df=20,\n",
    "                               tokenizer=tokenizer_stem)\n",
    "\n",
    "topics_tr_tf_26 = topics_tfidf_26.fit_transform(df_train_6[\"topics\"])\n",
    "topics_te_tf_26 = topics_tfidf_26.transform(df_test_6[\"topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## content\n",
    "ldaer_content_26 = LatentDirichletAllocation(n_components=100, n_jobs=2,max_iter=20)\n",
    "\n",
    "ldaer_content_26.fit(content_tr_tf_26)\n",
    "content_tr_lda_26 = ldaer_content_26.transform(content_tr_tf_26)\n",
    "content_te_lda_26 = ldaer_content_26.transform(content_te_tf_26)\n",
    "\n",
    "## title\n",
    "ldaer_title_26 = LatentDirichletAllocation(n_components=100, n_jobs=2)\n",
    "\n",
    "ldaer_title_26.fit(title_tr_tf_26)\n",
    "title_tr_lda_26 = ldaer_title_26.transform(title_tr_tf_26)\n",
    "title_te_lda_26 = ldaer_title_26.transform(title_te_tf_26)\n",
    "\n",
    "## topics\n",
    "ldaer_topic_26 = LatentDirichletAllocation(n_components=100, n_jobs=2)\n",
    "\n",
    "ldaer_topic_26.fit(topics_tr_tf_26)\n",
    "topic_tr_lda_26 = ldaer_topic_26.transform(topics_tr_tf_26)\n",
    "topic_te_lda_26 = ldaer_topic_26.transform(topics_te_tf_26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation2 (Method 4)\n",
    "接著，我將標題、內容、主題合併，並使用TF轉為字頻並使用LDA找出文章可能的主題，經過CV，主題個數選為100個。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_6[\"txt\"] = df_train_6[\"content\"] +\" \"+ df_train_6[\"title\"]+ \" \"+df_train_6[\"topics\"]\n",
    "df_test_6[\"txt\"] = df_test_6[\"content\"] +\" \"+ df_test_6[\"title\"]+ \" \"+df_test_6[\"topics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tfidf = CountVectorizer(ngram_range=(1, 1),\n",
    "                            min_df=20,\n",
    "                            max_df=0.4,\n",
    "                            tokenizer=tokenizer_stem)\n",
    "\n",
    "txt_tr_tf = txt_tfidf.fit_transform(df_train_6[\"txt\"])\n",
    "txt_te_tf = txt_tfidf.transform(df_test_6[\"txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 100\n",
    "ldaer_txt = LatentDirichletAllocation(n_components=n_top, n_jobs=2,max_iter=20)\n",
    "\n",
    "ldaer_txt.fit(txt_tr_tf)\n",
    "txt_tr_lda = ldaer_txt.transform(txt_tr_tf)\n",
    "txt_te_lda = ldaer_txt.transform(txt_te_tf)\n",
    "\n",
    "txt_names = [\"content_\"+str(i) for i in range(n_top)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF+max_feature (Method 5)\n",
    "在TF-IDF中挑選出最多100個字，避免太多字加進模型裡，效果不見得比較好，此外我們也計算這些字在文章出現的個數來當作重要變數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,1),min_df = 0.1,max_df = 0.2,max_features = 100,\n",
    "                        preprocessor=preprocessor_ta,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "tfidf.fit(X_train)\n",
    "feature_idf = tfidf.get_feature_names()\n",
    "pre_idf = []; pre_idf_test = []\n",
    "for i in range(len(X_train)):\n",
    "    pre_idf.append(tokenizer_stem_nostop(preprocessor_ta(X_train[i])))\n",
    "for i in range(len(X_test)):\n",
    "    pre_idf_test.append(tokenizer_stem_nostop(preprocessor_ta(X_test[i])))\n",
    "    \n",
    "idf = [];idf_test = []\n",
    "for i in range(len(X_train)):\n",
    "    num = 0\n",
    "    for j in range(len(tfidf.get_feature_names())):\n",
    "        num = num + pre_idf[i].count(feature_idf[j])\n",
    "    idf.append(num)\n",
    "for i in range(len(X_test)):\n",
    "    num = 0\n",
    "    for j in range(len(tfidf.get_feature_names())):\n",
    "        num = num + pre_idf_test[i].count(feature_idf[j])\n",
    "    idf_test.append(num)  \n",
    "    \n",
    "idf_num = [];idf_num_test = []\n",
    "for i in range(len(X_train)):\n",
    "    num = 0\n",
    "    for j in range(len(tfidf.get_feature_names())):\n",
    "        num = num + int(np.where((feature_idf[j] in pre_idf[i])==True,1,0))\n",
    "    idf_num.append(num)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    num = 0\n",
    "    for j in range(len(tfidf.get_feature_names())):\n",
    "        num = num + int(np.where((feature_idf[j] in pre_idf_test[i])==True,1,0))\n",
    "    idf_num_test.append(num) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合併資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop([\"content\",\"title\",\"topics\"],axis=1,inplace=True)\n",
    "df_test.drop([\"content\",\"title\",\"topics\"],axis=1,inplace=True)\n",
    "\n",
    "df_train_6.drop([\"content\",\"title\",\"topics\",\"txt\"],axis=1,inplace=True)\n",
    "df_test_6.drop([\"content\",\"title\",\"topics\",\"txt\"],axis=1,inplace=True)\n",
    "\n",
    "df_train_8.drop([\"content\",\"title\",\"topics\"],axis=1,inplace=True)\n",
    "df_test_8.drop([\"content\",\"title\",\"topics\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine model 1\n",
    "new_tr_18 = np.hstack((content_svd_tr_18, title_svd_tr_18, topics_svd_tr_18))\n",
    "new_te_18 = np.hstack((content_svd_te_18, title_svd_te_18, topics_svd_te_18))\n",
    "\n",
    "content_names = [\"content_\"+str(i) for i in range(1000)]\n",
    "title_names = [\"title_\"+str(i) for i in range(1000)]\n",
    "topics_names = [\"topics_\"+str(i) for i in range(500)]\n",
    "\n",
    "feature_names_18 = content_names+title_names+topics_names\n",
    "\n",
    "## combine model 2\n",
    "feature_names_22 = df_train_6.columns.to_list()\n",
    "feature_names_22.extend(title_feature_name_22+topics_feature_name_22)\n",
    "\n",
    "new_tr_22 = hstack((df_train_6.values, title_tr_tf_22, topics_tr_tf_22)).tocsr()\n",
    "new_te_22 = hstack((df_test_6.values, title_te_tf_22, topics_te_tf_22)).tocsr()\n",
    "\n",
    "## combine model 3\n",
    "new_tr_26 = np.hstack((content_tr_lda_26, title_tr_lda_26, topic_tr_lda_26))\n",
    "new_te_26 = np.hstack((content_te_lda_26, title_te_lda_26, topic_te_lda_26))\n",
    "\n",
    "content_names = [\"content_\"+str(i) for i in range(100)]\n",
    "title_names = [\"title_\"+str(i) for i in range(100)]\n",
    "topics_names = [\"topics_\"+str(i) for i in range(100)]\n",
    "\n",
    "feature_names_26 = content_names+title_names+topics_names\n",
    "\n",
    "## combine model 4\n",
    "feature_names_28 = df_train_6.columns.to_list()\n",
    "feature_names_28.extend(txt_names)\n",
    "\n",
    "new_tr_28 = pd.concat([df_train_6,pd.DataFrame(txt_tr_lda)],axis=1)\n",
    "new_te_28 = pd.concat([df_test_6,pd.DataFrame(txt_te_lda)],axis=1)\n",
    "\n",
    "## combine model 5\n",
    "feature_names_37 = df_train_8.columns.to_list()\n",
    "feature_names_37.extend(txt_names)\n",
    "\n",
    "new_tr_37 = pd.concat([df_train_8,pd.DataFrame(txt_tr_lda)],axis=1)\n",
    "new_te_37 = pd.concat([df_test_8,pd.DataFrame(txt_te_lda)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = pd.concat([month,year,week,hour,pd.DataFrame(article),\n",
    "                             YA,MY,WY,AP,DY,pd.DataFrame(pol),\n",
    "                             pd.DataFrame(idf*np.abs(pol))],axis = 1)\n",
    "X_test_combined = pd.concat([month_test,year_test,week_test,hour_test,pd.DataFrame(article_test),\n",
    "                            YA_test,MY_test,WY_test,AP_test,DY_test,pd.DataFrame(pol_test),\n",
    "                            pd.DataFrame(idf_test*np.abs(pol_test))],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Classifiers\n",
    "我們使用的模型包含LightGBM與XGBoost，這兩種方法皆為Gradient Boosting Tree的演算法，經過我們測試許多演算法，包含Ridge、RandomForest、SVM、KNN之後，這類方法的效果最好。\n",
    "<br/>我們透過使用Stacking的方式合併多個分類器，因此我們有許多不同的方法與Feature進行modeling的結果，合併的部分會在後面說明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_stack_lgb(param,tr,y,te,feature_names,cate_col=None,nfold=10):\n",
    "    kf = StratifiedKFold(n_splits=nfold, shuffle=True)\n",
    "    pred_tr = np.zeros(tr.shape[0])\n",
    "    pred_te = np.zeros(te.shape[0])\n",
    "\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    for fold_, (train_index, valid_index) in enumerate(kf.split(tr, y)):\n",
    "        if cate_col is None:\n",
    "            d_tr = lgb.Dataset(tr[train_index,:],\n",
    "                           y[train_index],\n",
    "                           feature_name=feature_names)\n",
    "        else:\n",
    "            d_tr = lgb.Dataset(tr[train_index,:],\n",
    "                               y[train_index],\n",
    "                               feature_name=feature_names,\n",
    "                               categorical_feature = cate_col)\n",
    "\n",
    "        clf = lgb.train(param, d_tr)\n",
    "        pred_tr[valid_index] = clf.predict(tr[valid_index,:])\n",
    "        pred_te += clf.predict(te)/nfold\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"Feature\"] = feature_names\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    return pred_tr, pred_te, feature_importance_df\n",
    "\n",
    "def quick_sklearn(model,tr,y,te,nfold=10):\n",
    "    kf = StratifiedKFold(n_splits=nfold, shuffle=True)\n",
    "    pred_tr = np.zeros(tr.shape[0])\n",
    "    pred_te = np.zeros(te.shape[0])\n",
    "    \n",
    "    for fold_, (train_index, valid_index) in enumerate(kf.split(tr, y)):\n",
    "        temp_tr = tr[train_index,:]\n",
    "        temp_tr_y = y[train_index]\n",
    "        temp_val = tr[valid_index,:]\n",
    "        \n",
    "        model.fit(temp_tr,temp_tr_y)\n",
    "        \n",
    "        pred_tr[valid_index] = model._predict_proba_lr(temp_val)[:,1]\n",
    "        pred_te += model._predict_proba_lr(te)[:,1]/nfold\n",
    "        \n",
    "    return pred_tr, pred_te\n",
    "\n",
    "def quick_stacking_xgb(train, y_train, test, n_fold=5):\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1)\n",
    "    tr_pred = np.zeros(train.shape[0])\n",
    "    te_pred = np.zeros(test.shape[0])\n",
    "    \n",
    "    for fold_, (tr_idx,val_idx) in enumerate(kfold.split(train,y_train)):\n",
    "        print(fold_+1)\n",
    "        tr_x = train.iloc[tr_idx,:]\n",
    "        tr_y = y_train[tr_idx]\n",
    "        val_x = train.iloc[val_idx,:]\n",
    "\n",
    "        model1 = XGBClassifier(n_estimators=300,learning_rate=0.01,max_depth = 8,colsample_bytree=0.7)\n",
    "        model1.fit(tr_x,tr_y)\n",
    "\n",
    "        tr_pred[val_idx] = model1.predict_proba(val_x)[:,1]\n",
    "        te_pred += model1.predict_proba(test)[:,1]/n_fold\n",
    "    \n",
    "    return tr_pred,te_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "我們使用Grid Search的方式來調整Hyperparameter，因為LightGBM與XGBoost都有Scikit-learn API與Training API，若使用Training API則會無法使用Scikit-learn的GridSearchCV，此時便要自己寫方法來進行調參，因此我自己寫了方法進行Grid Search。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def expand_grid(dictionary):\n",
    "    return pd.DataFrame([row for row in product(*dictionary.values())],\n",
    "                        columns=dictionary.keys())\n",
    "\n",
    "def convertType(param, typeDict):\n",
    "    inputParm = param.copy()\n",
    "\n",
    "    for key in inputParm.keys():\n",
    "        if typeDict[key] == np.int64:\n",
    "            inputParm[key] = int(inputParm[key])\n",
    "        elif typeDict[key] == np.float64:\n",
    "            inputParm[key] = float(inputParm[key])\n",
    "        else:\n",
    "            print(\"Not yet\")\n",
    "\n",
    "    return inputParm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"num_leaves\": 32,\n",
    "    'min_data_in_leaf': 50,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 1,\n",
    "    \"bagging_freq\":1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_boost_round\": 10000,\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"nthread\": 2\n",
    "}\n",
    "\n",
    "param_cv = {\n",
    "    'num_leaves': [20,32,40],\n",
    "    \"min_data_in_leaf\": [40,50,60,70],\n",
    "    \"feature_fraction\": [0.3,0.5,0.8,1],\n",
    "    \"bagging_fraction\":[0.6,0.8,1],\n",
    "}\n",
    "\n",
    "param_cv_pd = expand_grid(param_cv)\n",
    "theType = param_cv_pd.dtypes.to_dict()\n",
    "auc_list = []\n",
    "\n",
    "for i in tqdm(range(param_cv_pd.shape[0])):\n",
    "    d_all = lgb.Dataset(new_tr,y,\n",
    "                        feature_name=feature_names,\n",
    "                       categorical_feature = cate_col)\n",
    "    \n",
    "    nowParam = param_cv_pd.loc[i, :].to_dict()\n",
    "    nowParam = convertType(nowParam,theType)\n",
    "    print(\"\\n\", nowParam,\"\\n\")\n",
    "    \n",
    "    total_param = param.copy()\n",
    "    total_param.update(nowParam)\n",
    "    \n",
    "    clf_cv = lgb.cv(total_param,\n",
    "                    d_all,\n",
    "                    nfold=5,\n",
    "                    early_stopping_rounds=100,\n",
    "                    shuffle=True,\n",
    "                    metrics =\"auc\")\n",
    "    \n",
    "    auc_list.append(max(clf_cv[\"auc-mean\"]))\n",
    "    print(\"%.6f with %4d boost\" % (max(clf_cv[\"auc-mean\"]),len(clf_cv[\"auc-mean\"])))\n",
    "    gc.collect()\n",
    "    \n",
    "    if len(clf_cv[\"auc-mean\"])==total_param[\"num_boost_round\"]:\n",
    "        print(\"num_boost_round too low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_index = auc_list.index(max(auc_list))\n",
    "best_param = param_cv_pd.loc[top_index, :].to_dict()\n",
    "\n",
    "print(max(auc_list))\n",
    "print(best_param)\n",
    "\n",
    "param_cv_pd[\"AUC\"] = auc_list\n",
    "param_cv_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到合適的Hyperparameter後，便可進行之後的建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Model 1 (SVD+LightGBM)\n",
    "針對只使用TF-IDF+SVD萃取出的資料建模。測試過後發現若與其他Feature一起建模效果會不好，因此不使用任何先前Feature engineering的資料。\n",
    "<br/>這個方法的5-fold CV結果約為0.544628、Public Leadboard約為0.54489。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"num_leaves\": 32,\n",
    "    'min_data_in_leaf': 50,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\":1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_boost_round\": 109,\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"nthread\": 2\n",
    "}\n",
    "\n",
    "tr_pred_18, te_pred_18, featureImp = quick_stack_lgb(param=param,\n",
    "                                           tr=new_tr_18,\n",
    "                                           y=y,\n",
    "                                           te=new_te_18,\n",
    "                                           feature_names=feature_names_18,\n",
    "                                           nfold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (TFIDF+Features+LightGBM)\n",
    "因為內容的資料十分龐大，因此我嘗試只使用標題與主題各別計算TF-IDF並配上Feature engineering的Feature進行建模，效果意外的好。\n",
    "<br/>這個方法的5-fold CV結果約為0.607653、Public Leadboard約為0.59039。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"num_leaves\": 32,\n",
    "    'min_data_in_leaf': 50,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 1,\n",
    "    \"bagging_freq\":1,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"num_boost_round\": 461,\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"nthread\": 2,\n",
    "    \"random_state\":192\n",
    "}\n",
    "\n",
    "tr_pred_22, te_pred_22, featureImp = quick_stack_lgb(param=param,\n",
    "                                           tr=new_tr_22,\n",
    "                                           y=y,\n",
    "                                           te=new_te_22,\n",
    "                                           feature_names=feature_names_22,\n",
    "                                           cate_col=cate_col,\n",
    "                                           nfold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3  (LDA1+LightGBM)\n",
    "根據我的觀察，長文章容易被判定為有人氣，但有部分有人氣的文章卻因為是短文章而被給予很低的機率，因此我認為若要再提升成績，可能要以文章內容著手，而我覺得可能會造成影響的是文章內容本身的主題，不單單是文章的標籤主題。\n",
    "<br/>因此我嘗試只使用LDA萃取出的主題資料，並根據先前SVD的經驗先不使用任何先前Feature engineering的資料變建模。\n",
    "<br/>這個方法的5-fold CV結果約為0.545384，沒有將此結果上傳過，因此Public Leadboard未知。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"num_leaves\": 32,\n",
    "    'min_data_in_leaf': 50,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 1,\n",
    "    \"bagging_freq\":1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_boost_round\": 287,\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"nthread\": 2\n",
    "}\n",
    "\n",
    "tr_pred_26, te_pred_26, featureImp = quick_stack_lgb(param=param,\n",
    "                                           tr=new_tr_26,\n",
    "                                           y=y,\n",
    "                                           te=new_te_26,\n",
    "                                           feature_names=feature_names_26,\n",
    "                                           nfold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4  (LDA2+Features+LightGBM)\n",
    "接著，我再使用LDA萃取出的主題資料加上Feature engineering的Feature進行建模，AUC有明顯的提升，與我先前的猜測相符。\n",
    "<br/>這個方法的5-fold CV結果約為0.608212、Public Leadboard約為0.59314。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"num_leaves\": 32,\n",
    "    'min_data_in_leaf': 20,\n",
    "    \"feature_fraction\": 0.5,\n",
    "    \"bagging_fraction\": 1,\n",
    "    \"bagging_freq\":1,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"num_boost_round\": 847,\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"nthread\": 2,\n",
    "    \"random_state\":192\n",
    "}\n",
    "\n",
    "tr_pred_28, te_pred_28, featureImp = quick_stack_lgb(param=param,\n",
    "                                           tr=new_tr_28.to_numpy(),\n",
    "                                           y=y,\n",
    "                                           te=new_te_28.to_numpy(),\n",
    "                                           feature_names=feature_names_28,\n",
    "                                           nfold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 (LDA2+Features+LightGBM)\n",
    "因為我發現文章時間至年底的間隔的Feature與文章人氣有明顯相關，所以我使用LDA萃取出的主題資料加上Feature engineering的Feature並額外再加上文章時間至年底的間隔的Feature，以進行建模，雖然Leadboard的成績沒有上升，但Cross-validation卻有上升，因此我還是選擇將其保留。\n",
    "<br/>這個方法的5-fold CV結果約為0.60853、Public Leadboard約為0.58926。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"num_leaves\": 32,\n",
    "    'min_data_in_leaf': 20,\n",
    "    \"feature_fraction\": 0.5,\n",
    "    \"bagging_fraction\": 1,\n",
    "    \"bagging_freq\":1,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"num_boost_round\": 537,\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"nthread\": 2\n",
    "}\n",
    "\n",
    "tr_pred_37, te_pred_37, featureImp = quick_stack_lgb(param=param,\n",
    "                                           tr=new_tr_37.to_numpy(),\n",
    "                                           y=y,\n",
    "                                           te=new_te_37.to_numpy(),\n",
    "                                           feature_names=feature_names_37,\n",
    "                                           nfold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6 (max_feature_100+Features+XGBoost)\n",
    "使用XGBoost對於使用max_feature為100的資料配上Feature engineering的Feature進行建模。\n",
    "<br/>這個方法的5-fold CV結果約為0.589288。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred_yang, te_pred_yang = quick_stacking_xgb(X_train_combined,y,X_test_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "我們有嘗試過進行Feature selection，但效果不進理想，不只沒有提升AUC，反而還使AUC稍微降低，因此沒有將結果納入最後的結果中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"num_leaves\": 32,\n",
    "    'min_data_in_leaf': 50,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\":1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_boost_round\": 162,\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"nthread\": 2,\n",
    "    \"random_state\":192\n",
    "}\n",
    "\n",
    "d_all = lgb.Dataset(new_tr,y,\n",
    "                    feature_name=feature_names,\n",
    "                    categorical_feature = cate_col)\n",
    "\n",
    "lgb_clf = lgb.train(param, d_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(important, all_feature):\n",
    "    output = []\n",
    "    output_name = []\n",
    "    imp_set = set(important)\n",
    "    \n",
    "    for i in range(len(all_feature)):\n",
    "        if all_feature[i] in imp_set:\n",
    "            output.append(i)\n",
    "            output_name.append(all_feature[i])\n",
    "\n",
    "    return output, output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_index, imp_name = select_feature(temp[\"Feature\"].tolist(),feature_names)\n",
    "\n",
    "output_tr = new_tr[:,imp_index].todense()\n",
    "output_te = new_te[:,imp_index].todense()\n",
    "\n",
    "output_tr = pd.DataFrame(output_tr)\n",
    "output_te = pd.DataFrame(output_te)\n",
    "\n",
    "output_tr.columns = imp_name\n",
    "output_te.columns = imp_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "我們透過使用Stacking的方式合併多個分類器，其原理類似於voting，但權重是使用Cross-validation算出的資料再使用其他模型訓練出來，因此其權重會相較單純取平均要更好許多。\n",
    "<br/>因為Stacking的模型不需要太複雜即可有很好的結果，因此我們使用Ridge Regression進行stacking。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_selected = pd.DataFrame()\n",
    "te_selected = pd.DataFrame()\n",
    "\n",
    "for ind, (tr,te) in enumerate(zip([tr_pred_18,tr_pred_22,tr_pred_26,tr_pred_28,tr_pred_37],\n",
    "                                  [te_pred_18,te_pred_22,te_pred_26,te_pred_28,te_pred_37])):\n",
    "    tr_selected[str(ind)] = tr\n",
    "    te_selected[str(ind)] = te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifierCV(alphas=np.arange(0.001, 2, 0.005),\n",
    "                        cv=10,\n",
    "                        scoring=\"roc_auc\",\n",
    "                        normalize=True,\n",
    "                        fit_intercept=True)\n",
    "clf.fit(tr_selected, y)\n",
    "\n",
    "clf_ridge = RidgeClassifier(alpha=clf.alpha_,\n",
    "                            normalize=True,\n",
    "                            fit_intercept=True)\n",
    "\n",
    "tr_pred, te_pred = quick_sklearn(clf_ridge,\n",
    "                                tr_selected.to_numpy(),\n",
    "                                y,\n",
    "                                te_selected.to_numpy(),\n",
    "                                10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv(\"./data/original/sample_submission.csv\")\n",
    "output[\"Popularity\"]=te_pred\n",
    "output.to_csv(\"./result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此方法的10-Fold CV AUC為0.61140，比先前的幾個模型都要來得好，因此使用其為最終結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "在這次的競賽中，我們認為有趣的點、學習與觀察到的陷阱如以下：\n",
    "1. 原始文字資料中可能會出現前述的javascript的陷阱，必須要靠一篇一篇看得方式才能夠看出，表示我們就算有這些厲害的演算法，仍然需要多加觀察資料本身。\n",
    "2. 即便原始分數沒有很高的方法，透過stacking或voting的方式仍然可以對最後的結果有幫助，這顯現了ensemble learning方法的精隨。\n",
    "3. Feature engineering的過程可以提高AUC許多，讓我了解到其重要性。\n",
    "4. SVD若直接與數值Feature一起建模效果很差，但若只用SVD的資料建模，最後再與其他資料合併，效果卻會很好。讓我了解到不要一個方法看起來沒有效果就直接捨棄，將其用不同方式利用可能有意想不到的效果。 \n",
    "5. 最後的Kaggle排名有十分明顯的波動，表示可能有些人掉入了overfitting的陷阱中，讓我更了解到以後必須警惕。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
